Project file structure:
=======================
./
    app.ts
    brain.ts
    server.ts
    middleware/
        errorHandler.ts
    types/
        tree-sitter-typescript.d.ts
    core/
        chunker.ts
        documentExtractor.ts
        textChunker.ts
        prompts/
            fileSummary.prompt.ts
            taskGeneration.prompt.ts
    scripts/
        ingest.ts
    api/
        tasks/
            task.controller.ts
            task.routes.ts
            task.service.ts
        projects/
            project.controller.ts
            project.routes.ts
            project.service.ts
            qa.service.ts
    services/
        db.ts
        git.ts
        openai.ts
        queue.service.ts


File Contents:
===============


--- FILE: app.ts ---

// src/app.ts
import 'dotenv/config';
import express from 'express';
import path from 'path';
import projectRoutes from './api/projects/project.routes';
import cors from 'cors';
import { errorHandler } from './middleware/errorHandler';

const app = express();
app.use(cors());

// Middleware
app.use(express.json());
app.use(express.static(path.join(process.cwd(), 'public')));

// API Routes
app.use('/api/projects', projectRoutes);

// Error Handler
app.use(errorHandler);

export default app;

--- FILE: brain.ts ---

#!/usr/bin/env node
// --- FILE: brain.ts ---

import 'dotenv/config';
import { Command } from 'commander';
import { Client } from 'pg';
import pgvector from 'pgvector/pg';
import OpenAI from 'openai';
import { runIngestion } from './scripts/ingest';
import simpleGit, { SimpleGit } from 'simple-git';
import { promises as fs } from 'fs';
import path from 'path';
import os from 'os';

// --- CONFIGURATION ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;
const WORKSPACE_DIR = path.join(os.homedir(), '.ai-brain-workspace');

if (!connectionString || !openaiApiKey) {
    throw new Error("FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY");
}
const openai = new OpenAI({ apiKey: openaiApiKey });
const program = new Command();

// --- HELPER FUNCTIONS ---

/**
 * Retrieves the ID for a project from the database based on its source (Git URL or path).
 * If the project doesn't exist, it creates a new one.
 * @param source The unique Git URL or local path for the project.
 * @param client The active Postgres client.
 * @returns The numeric ID of the project.
 */
async function getProjectId(source: string, client: Client): Promise<number> {
    const projectRes = await client.query('SELECT id FROM projects WHERE source = $1', [source]);
    if (projectRes.rows.length > 0) {
        return projectRes.rows[0].id;
    } else {
        const projectName = path.basename(source, path.extname(source));
        console.log(`‚ú® Creating new project entry for '${projectName}'...`);
        const newProjectRes = await client.query(
            'INSERT INTO projects (name, source) VALUES ($1, $2) RETURNING id',
            [projectName, source]
        );
        return newProjectRes.rows[0].id;
    }
}

async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

function getWorkspacePathFromUrl(url: string): string {
    try {
        const parsedUrl = new URL(url);
        const cleanPath = (parsedUrl.hostname + parsedUrl.pathname).replace(/\.git$/, '');
        return path.join(WORKSPACE_DIR, cleanPath);
    } catch (e) {
        const sshMatch = url.match(/git@([^:]+):(.*)/);
        if (sshMatch) {
            const host = sshMatch[1];
            const repoPath = sshMatch[2].replace(/\.git$/, '');
            return path.join(WORKSPACE_DIR, host, repoPath);
        }
        return path.join(WORKSPACE_DIR, url.replace(/[^a-zA-Z0-9]/g, '_'));
    }
}

// --- CLI COMMAND DEFINITIONS ---

program
  .command('ingest')
  .description('Ingest a project from a local path or a public Git URL.')
  .argument('<source>', 'The local path or Git URL of the project')
  .action(async (source: string) => {
    let projectPath = source;

    if (source.startsWith('http') || source.startsWith('git@')) {
        projectPath = getWorkspacePathFromUrl(source);
        try {
            await fs.mkdir(WORKSPACE_DIR, { recursive: true });
            try {
                await fs.access(path.join(projectPath, '.git'));
                console.log(`üß† Found existing repository. Fetching updates from ${source}...`);
                await simpleGit(projectPath).pull();
                console.log(`   -> Updates pulled successfully.`);
            } catch (error) {
                console.log(`üß† Cloning repository from ${source}...`);
                await simpleGit().clone(source, projectPath);
                console.log(`   -> Cloned successfully into: ${projectPath}`);
            }
        } catch (gitError) {
            console.error('‚ùå A Git error occurred:', gitError);
            process.exit(1);
        }
    }

    const client = new Client({ connectionString });
    try {
      await client.connect();
      const projectId = await getProjectId(source, client);
      await runIngestion(projectId, projectPath, console.log);
      console.log('‚úÖ Ingestion complete.');
    } catch (error) {
      console.error('‚ùå Ingestion failed:', error);
      process.exit(1);
    } finally {
      await client.end();
    }
  });

program
  .command('ask')
  .description('Ask a question about the indexed codebase.')
  .argument('<question>', 'The question to ask')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .action(async (question: string, options: { project: string }) => {
    console.log(`üß† Thinking about: "${question}"`);
    const client = new Client({ connectionString });
    try {
      await client.connect();
      await pgvector.registerType(client);
      const projectId = await getProjectId(options.project, client);

      const questionEmbedding = await getEmbedding(question);
      let contextString = '';

      // --- Retrieve relevant tasks ---
      const { rows: relevantTasks } = await client.query(
        `SELECT task_number, title, status FROM tasks WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );

      if (relevantTasks.length > 0) {
        console.log(`\nüîç Found relevant tasks: ${relevantTasks.map(t => `#${t.task_number}`).join(', ')}`);
        contextString += "Relevant Tasks:\n" + relevantTasks.map(t => `- Task #${t.task_number} [${t.status.toUpperCase()}]: ${t.title}`).join('\n') + '\n\n';
      }

      // --- Retrieve relevant commits ---
      const { rows: relevantCommits } = await client.query(
        `SELECT commit_hash, message, author_name FROM commits WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );
      
      if (relevantCommits.length > 0) {
          console.log(`\nüîç Found relevant commits: ${relevantCommits.map(c => c.commit_hash.substring(0, 7)).join(', ')}`);
          contextString += "Relevant Commits:\n" + relevantCommits.map(c => `- Commit ${c.commit_hash.substring(0, 7)} by ${c.author_name}: ${c.message.split('\n')[0]}`).join('\n') + '\n\n';
      }

      // --- Retrieve relevant files and code chunks ---
      const { rows: relevantFiles } = await client.query(
        `SELECT id, path, summary FROM indexed_files WHERE project_id = $1 ORDER BY summary_embedding <=> $2 LIMIT 5`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );
      
      if (relevantFiles.length === 0 && relevantTasks.length === 0 && relevantCommits.length === 0) {
        console.log("I couldn't find any relevant information to answer that question.");
        return;
      }
      
      if (relevantFiles.length > 0) {
        console.log(`\nüîç Found relevant files: ${relevantFiles.map(f => f.path).join(', ')}`);
        const relevantFileIds = relevantFiles.map(f => f.id);

        const { rows: contextChunks } = await client.query(
          `SELECT file_id, content, chunk_name FROM code_chunks WHERE file_id = ANY($1::int[]) ORDER BY embedding <=> $2 LIMIT 10`,
          [relevantFileIds, pgvector.toSql(questionEmbedding)]
        );

        if (contextChunks.length > 0) {
          const chunkContext = contextChunks.map(c => {
            const filePath = relevantFiles.find(f => f.id === c.file_id)?.path;
            return `--- FILE: ${filePath} (Chunk: ${c.chunk_name}) ---\n\n${c.content}`;
          }).join('\n\n');
          contextString += "Relevant Code Snippets:\n" + chunkContext;
        }
      }

      if (!contextString.trim()) {
        console.log("I found some potentially relevant items but couldn't construct a concrete context to answer your question.");
        return;
      }

      const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided context, which may include tasks, commits, and code snippets. Be concise, accurate, and provide code snippets in Markdown format when relevant. If the context is insufficient, state that clearly.`;
      const userPrompt = `CONTEXT:\n${contextString}\n\nQUESTION:\n${question}`;

      const stream = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
        stream: true,
      });
      
      console.log('\nüí¨ Answer:\n');
      for await (const chunk of stream) {
        process.stdout.write(chunk.choices[0]?.delta?.content || '');
      }
      console.log('\n');

    } catch (error) {
      console.error('‚ùå An error occurred:', error);
    } finally {
      await client.end();
    }
  });

// --- TASK MANAGEMENT COMMANDS ---
const task = program.command('task').description('Manage project tasks');

task
  .command('add')
  .description('Add a new task to a project')
  .argument('<title>', 'The title of the task')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .action(async (title: string, options: { project: string }) => {
    const client = new Client({ connectionString });
    try {
        await client.connect();
        const projectId = await getProjectId(options.project, client);
        
        const titleEmbedding = await getEmbedding(title);

        const result = await client.query(
            'INSERT INTO tasks (project_id, title, embedding) VALUES ($1, $2, $3) RETURNING task_number',
            [projectId, title, pgvector.toSql(titleEmbedding)]
        );
        const taskNumber = result.rows[0].task_number;
        console.log(`‚úÖ Created task #${taskNumber}: "${title}"`);
    } catch (error) {
        console.error('‚ùå Could not add task:', error);
        console.error('NOTE: This might be due to a missing `embedding` column on the `tasks` table. Please run: ALTER TABLE tasks ADD COLUMN embedding vector(1536);');
    } finally {
        await client.end();
    }
  });

task
  .command('list')
  .description('List tasks for a project')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .option('--status <status>', 'Filter by status (e.g., open, done)', 'open')
  .action(async (options: { project: string, status: string }) => {
    const client = new Client({ connectionString });
    try {
        await client.connect();
        const projectId = await getProjectId(options.project, client);
        const { rows } = await client.query(
            'SELECT task_number, title, status FROM tasks WHERE project_id = $1 AND status = $2 ORDER BY task_number ASC',
            [projectId, options.status]
        );

        if (rows.length === 0) {
            console.log(`No '${options.status}' tasks found for project: ${options.project}`);
            return;
        }

        console.log(`\nTasks for project: ${options.project} [Status: ${options.status}]`);
        console.log('--------------------------------------------------');
        rows.forEach(t => {
            const status = `[${t.status.toUpperCase()}]`.padEnd(7);
            console.log(`#${t.task_number.toString().padEnd(4)} ${status} ${t.title}`);
        });
        console.log('--------------------------------------------------');
    } catch (error) {
        console.error('‚ùå Could not list tasks:', error);
    } finally {
        await client.end();
    }
  });

program.parse(process.argv);

--- FILE: server.ts ---

// src/server.ts
import app from './app';

const port = process.env.PORT || 3000;

app.listen(port, () => {
    console.log(`üß† AI Project Brain is listening at http://localhost:${port}`);
});

--- FILE: middleware/errorHandler.ts ---

// src/middleware/errorHandler.ts
import { Request, Response, NextFunction } from 'express';

export function errorHandler(err: Error, req: Request, res: Response, next: NextFunction) {
    console.error(err.stack);
    res.status(500).json({ error: err.message || 'An internal server error occurred.' });
}

--- FILE: types/tree-sitter-typescript.d.ts ---

declare module 'tree-sitter-typescript/typescript';

--- FILE: core/chunker.ts ---

// --- FILE: core/chunker.ts ---
import Parser, { Query } from 'tree-sitter'; // MODIFIED: Import the Query class
import TypeScript from 'tree-sitter-typescript/typescript';

export interface CodeChunk {
  content: string;
  metadata: {
    type: 'function' | 'class' | 'method' | 'arrow_function' | 'block';
    name: string;
    start_line: number;
    end_line: number;
  };
}

const parser = new Parser();
parser.setLanguage(TypeScript);

// CRITICAL: This query identifies the distinct, semantic blocks of code.
const TS_QUERY = `
[
  (function_declaration) @chunk
  (class_declaration) @chunk
  (method_definition) @chunk
  (lexical_declaration 
    (variable_declarator 
      value: (arrow_function)
    )
  ) @chunk
]
`;

export function chunkCodeWithAST(content: string): CodeChunk[] {
  const tree = parser.parse(content);
  // MODIFIED: Use the Query constructor for robustness
  const query = new Query(TypeScript, TS_QUERY);
  const matches = query.captures(tree.rootNode);

  const chunks: CodeChunk[] = [];

  for (const match of matches) {
    const node = match.node;
    let name = 'anonymous';
    let type: CodeChunk['metadata']['type'] = 'block';

    // Heuristics to find the name of the chunk
    if (node.type === 'function_declaration' || node.type === 'class_declaration' || node.type === 'method_definition') {
        name = node.childForFieldName('name')?.text || 'anonymous';
        if (node.type === 'method_definition') type = 'method';
        else if (node.type === 'class_declaration') type = 'class';
        else type = 'function';
    } else if (node.type === 'lexical_declaration') {
        // For arrow functions like `const myFunc = () => ...`
        name = node.firstNamedChild?.firstNamedChild?.text || 'anonymous_arrow_function';
        type = 'arrow_function';
    }
    
    chunks.push({
      content: node.text,
      metadata: {
        type: type,
        name: name,
        start_line: node.startPosition.row + 1,
        end_line: node.endPosition.row + 1,
      },
    });
  }

  // Fallback: If no specific chunks were found (e.g., a simple config file),
  // treat the entire file as a single chunk.
  if (chunks.length === 0 && content.trim().length > 0) {
    chunks.push({
      content: content,
      metadata: {
        type: 'block',
        name: 'file_content',
        start_line: 1,
        end_line: content.split('\n').length,
      },
    });
  }

  return chunks;
}

--- FILE: core/documentExtractor.ts ---

// --- FILE: core/documentExtractor.ts ---
import fs from 'fs';
import path from 'path';
import mammoth from 'mammoth';
import pdf from 'pdf-parse';
import TurndownService from 'turndown';

export class UnsupportedFileTypeError extends Error {
    constructor(message: string) {
        super(message);
        this.name = 'UnsupportedFileTypeError';
    }
}

// MODIFIED: Create a Turndown instance and add a custom rule.
const turndownService = new TurndownService();

// This new rule identifies paragraphs that ONLY contain bold text and treats them as H2 headings.
// This is a robust way to handle documents where users bold text for headers instead of using proper styles.
turndownService.addRule('strongIsHeading', {
    filter: (node, options) => {
        // Check if the node is a paragraph
        if (node.nodeName !== 'P') {
            return false;
        }
        // Check if it has exactly one child
        if (node.childNodes.length !== 1) {
            return false;
        }
        const child = node.firstChild!;
        // Check if that single child is a <strong> tag
        if (child.nodeName !== 'STRONG') {
            return false;
        }
        // Optional: Check if the heading text is reasonably short
        return (child.textContent || '').length < 200;
    },
    replacement: (content) => {
        // Replace it with a markdown H2 heading
        return `## ${content}\n\n`;
    }
});


/**
 * Extracts clean text content from a file based on its extension.
 * Supports .txt, .md, .docx, and .pdf files.
 * @param filePath The path to the file on disk (the temporary file).
 * @param originalFilename The original name of the file, used to determine the extension.
 * @returns A promise that resolves with the extracted text content.
 * @throws {UnsupportedFileTypeError} if the file extension is not supported.
 */
export async function extractTextFromFile(filePath: string, originalFilename: string): Promise<string> {
    const extension = path.extname(originalFilename).toLowerCase();
    const buffer = fs.readFileSync(filePath);

    switch (extension) {
        case '.docx':
            // The styleMap is a good fallback, but our new Turndown rule is more robust for this case.
            const options = {
                styleMap: [
                    "p[style-name='Heading 1'] => h1:fresh",
                    "p[style-name='Heading 2'] => h2:fresh",
                    "p[style-name='Heading 3'] => h3:fresh",
                    "p[style-name='heading 1'] => h1:fresh",
                    "p[style-name='heading 2'] => h2:fresh",
                    "p[style-name='heading 3'] => h3:fresh",
                ]
            };
            const htmlResult = await mammoth.convertToHtml({ buffer }, options);
            // Use our customized turndown service to convert HTML to Markdown
            return turndownService.turndown(htmlResult.value);

        case '.pdf':
            const data = await pdf(buffer);
            return data.text.replace(/(\s*\n){3,}/g, '\n\n').trim();

        case '.txt':
        case '.md':
        case '.json':
        case '.ts':
        case '.js':
        case '.py':
        case '.html':
        case '.css':
            return buffer.toString('utf-8');
            
        default:
            throw new UnsupportedFileTypeError(`File type "${extension}" is not supported for document ingestion.`);
    }
}

--- FILE: core/textChunker.ts ---

// --- FILE: core/textChunker.ts ---
import { remark } from 'remark';
import { toString } from 'mdast-util-to-string';
import { Root } from 'mdast';

/**
 * A fallback chunker for plain text with no discernible structure.
 * It splits the content by paragraphs (double newlines).
 */
function chunkByParagraphs(content: string): string[] {
    return content
        .split(/\n\s*\n/)
        .map(chunk => chunk.trim())
        .filter(chunk => chunk.length > 20); // Filter out very short lines
}

/**
 * A sophisticated chunker that understands Markdown structure.
 * It groups content under headings to create semantically meaningful chunks.
 * For example, a "## Section Title" and all its following paragraphs and lists
 * will be combined into a single chunk.
 */
function chunkByHeadings(markdownContent: string): string[] {
    const tree = remark().parse(markdownContent) as Root;
    const chunks: string[] = [];
    let currentChunk: any[] = [];

    tree.children.forEach(node => {
        // A heading of level 1, 2, or 3 now starts a new chunk for better granularity.
        if (node.type === 'heading' && node.depth <= 3) {
            if (currentChunk.length > 0) {
                chunks.push(toString({ type: 'root', children: currentChunk }));
            }
            currentChunk = [node];
        } else {
            currentChunk.push(node);
        }
    });

    if (currentChunk.length > 0) {
        chunks.push(toString({ type: 'root', children: currentChunk }));
    }

    // --- NEW LOGGING: START ---
    console.log(`[textChunker] Initial chunking pass found ${chunks.length} potential chunks based on headings.`);
    // --- NEW LOGGING: END ---

    // If the document had no headings, the whole thing is one chunk.
    // In that case, we fall back to the paragraph chunker for better granularity.
    if (chunks.length === 1 && tree.children.some(node => node.type !== 'heading')) {
        // --- NEW LOGGING: START ---
        console.log('[textChunker] Only one heading-based chunk found. Falling back to paragraph-based chunking for better granularity.');
        // --- NEW LOGGING: END ---
        return chunkByParagraphs(chunks[0]);
    }
    
    return chunks.filter(chunk => chunk.trim().length > 0);
}

/**
 * The main text chunking function. It attempts to chunk by Markdown headings first,
 * and falls back to a simpler paragraph-based chunker if needed.
 * @param content The text or markdown content to be chunked.
 * @returns An array of string chunks.
 */
export function chunkText(content: string): string[] {
    // Trim initial/final whitespace which can affect parsing
    const trimmedContent = content.trim();
    if (!trimmedContent) {
        return [];
    }
    return chunkByHeadings(trimmedContent);
}

--- FILE: core/prompts/fileSummary.prompt.ts ---

// --- FILE: core/prompts/fileSummary.prompt.ts ---

/**
 * Generates the prompt for summarizing a single code file.
 * @param filePath The relative path of the file.
 * @param content The full content of the file.
 * @returns A formatted string ready to be sent to the OpenAI API.
 */
export function generateFileSummaryPrompt(filePath: string, content: string): string {
    return `Summarize the purpose of the following code file in one sentence. Be concise and focus on the file's primary role or responsibility.

File Path: ${filePath}

--- CODE START ---
${content}
--- CODE END ---

One-sentence summary:`;
}

--- FILE: core/prompts/taskGeneration.prompt.ts ---

// --- FILE: core/prompts/taskGeneration.prompt.ts ---

/**
 * Generates the full prompt for the AI to analyze a git commit and create a structured task.
 * @param commitMessage The message from the git commit.
 * @param gitDiff The full text diff of the changes in the commit.
 * @returns A formatted string ready to be sent to the OpenAI API.
 */
export function generateTaskFromCommitPrompt(commitMessage: string, gitDiff: string): string {
    return `You are an expert software engineering project manager analyzing a git commit. Your goal is to generate a concise task title, a category, and a detailed description of the work.

**Instructions & Rules:**

1.  **Analyze**: Carefully review the commit message and the git diff to understand the full context of the work performed.
2.  **Output Format**: You MUST respond ONLY with a single JSON object. Do not include any explanatory text, markdown syntax, or anything outside of the JSON structure.
3.  **Trivial Commits**: If the diff is truly trivial (e.g., only a typo fix in a comment, a whitespace change), respond with the exact string "NULL" instead of a JSON object.
4.  **JSON Structure**: The JSON object must have three keys: "title", "category", and "description".

    *   **"title" field rules:**
        *   MUST start with an imperative verb (e.g., "Add", "Refactor", "Fix", "Update", "Implement").
        *   MUST be a single, concise line summarizing the work done.

    *   **"category" field rules:**
        *   MUST be one of the following exact strings: 'feature', 'fix', 'refactor', 'chore', 'docs', 'test'.

    *   **"description" field rules:**
        *   MUST be a brief, one or two-sentence summary of the changes.
        *   This summary SHOULD be followed by a short markdown bulleted list of the most important changes.
        *   Example: "This commit introduces a new queueing system to handle concurrent ingestion tasks. Key changes include:\n- Added 'p-queue' library.\n- Created a singleton queue service.\n- Wrapped the ingestion logic in the project controller."

**JSON Output Structure Example:**
\`\`\`json
{
  "title": "Implement a queue for ingestion tasks",
  "category": "feature",
  "description": "Introduces a job queue to manage concurrent project sync requests, preventing system crashes under load. Key changes include:\\n- Added 'p-queue' dependency.\\n- Created a global singleton queue service with a concurrency of 1.\\n- Modified the project controller to add ingestion jobs to the queue."
}
\`\`\`

---
**INPUT DATA**
---

**COMMIT MESSAGE:**
\`\`\`
${commitMessage}
\`\`\`

**GIT DIFF:**
\`\`\`diff
${gitDiff}
\`\`\`
`;
}

--- FILE: scripts/ingest.ts ---

// --- FILE: scripts/ingest.ts ---

import 'dotenv/config';
import { glob } from 'glob';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { Client } from 'pg';
import OpenAI from 'openai';
import gitignore from 'gitignore-parser';
import pgvector from 'pgvector/pg';
import { chunkCodeWithAST } from '../core/chunker';
import simpleGit, { SimpleGit, LogResult, DefaultLogFields } from 'simple-git';
// REFACTORED: Import both dedicated prompt generators
import { generateFileSummaryPrompt } from '../core/prompts/fileSummary.prompt';
import { generateTaskFromCommitPrompt } from '../core/prompts/taskGeneration.prompt';

// --- CONFIGURATION (Global) ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;

if (!connectionString || !openaiApiKey) {
  throw new Error('FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY');
}

const openai = new OpenAI({ apiKey: openaiApiKey });
const IGNORED_EXTENSIONS = new Set(['.lock', '.svg', '.png', '.jpg', '.jpeg', '.gif', '.ico']);
const IGNORED_FILENAMES = new Set(['package-lock.json', 'yarn.lock', 'pnpm-lock.yaml']);

export type IngestionLogger = (message: string) => void;

// --- CORE HELPER FUNCTIONS ---
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

// REFACTORED: This function now uses the imported prompt
async function summarizeFile(filePath: string, content: string, logger: IngestionLogger): Promise<string> {
  const prompt = generateFileSummaryPrompt(filePath, content);
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 100,
      temperature: 0.1,
    });
    return response.choices[0].message.content?.trim() || "Could not generate a summary.";
  } catch (error) {
    logger(`  - Failed to summarize ${filePath}: ${error instanceof Error ? error.message : String(error)}`);
    return "Summary generation failed.";
  }
}

// --- MAIN INGESTION LOGIC ---
export async function runIngestion(projectId: number, projectPath: string, logger: IngestionLogger) {
  const client = new Client({ connectionString });
  await client.connect();
  await pgvector.registerType(client);
  logger('Database connection established.');

  const git: SimpleGit = simpleGit(projectPath);

  try {
    await syncFiles(client, projectId, projectPath, logger);
    await syncGitHistory(client, projectId, git, logger);
  } finally {
    logger('Ingestion process finished. Closing database connection.');
    await client.end();
  }
}

// --- STAGE 1: Sync Filesystem State ---
// This function remains the same, but its call to summarizeFile is now cleaner.
async function syncFiles(client: Client, projectId: number, projectPath: string, logger: IngestionLogger) {
  logger(`[1/4] Starting file sync for project ID: ${projectId}`);
  
  logger(`[2/4] Pruning deleted files from the database...`);
  const { rows: dbFiles } = await client.query('SELECT path FROM indexed_files WHERE project_id = $1', [projectId]);
  const dbPaths = new Set(dbFiles.map(f => f.path));
  
  const allDiskFiles = await glob('**/*', { cwd: projectPath, nodir: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] });
  const diskPaths = new Set(allDiskFiles);

  const pathsToDelete = [...dbPaths].filter(p => !diskPaths.has(p));

  if (pathsToDelete.length > 0) {
    logger(`      Found ${pathsToDelete.length} files to delete.`);
    await client.query('DELETE FROM indexed_files WHERE project_id = $1 AND path = ANY($2::text[])', [projectId, pathsToDelete]);
    logger(`      -> Pruning complete.`);
  } else {
    logger(`      -> No files to prune.`);
  }

  const gitignorePath = path.join(projectPath, '.gitignore');
  const ignore = fs.existsSync(gitignorePath)
    ? gitignore.compile(fs.readFileSync(gitignorePath, 'utf8'))
    : { accepts: (_p: string) => true };

  const filesToIndex = allDiskFiles.filter(file => {
    const ext = path.extname(file);
    const filename = path.basename(file);
    return ignore.accepts(file) && !IGNORED_EXTENSIONS.has(ext) && !IGNORED_FILENAMES.has(filename);
  });

  logger(`[3/4] Found ${filesToIndex.length} files to process for additions/modifications.`);
  let processedCount = 0;

  for (const relativePath of filesToIndex) {
    const fullPath = path.join(projectPath, relativePath);
    const content = fs.readFileSync(fullPath, 'utf-8');
    if (!content.trim()) continue;

    const hash = crypto.createHash('sha256').update(content).digest('hex');
    const { rows } = await client.query('SELECT content_hash FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, relativePath]);

    if (rows.length > 0 && rows[0].content_hash === hash) {
      continue;
    }

    processedCount++;
    logger(`      Processing changed file: ${relativePath}`);

    await client.query('BEGIN');
    try {
      await client.query('DELETE FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, relativePath]);
      
      const summary = await summarizeFile(relativePath, content, logger);
      const summaryEmbedding = await getEmbedding(summary);
      
      const fileInsertResult = await client.query(
        'INSERT INTO indexed_files (project_id, path, content_hash, summary, summary_embedding, last_indexed_at) VALUES ($1, $2, $3, $4, $5, NOW()) RETURNING id',
        [projectId, relativePath, hash, summary, pgvector.toSql(summaryEmbedding)]
      );
      const fileId = fileInsertResult.rows[0].id;
      
      const chunks = chunkCodeWithAST(content);
      for (const chunk of chunks) {
        const chunkEmbedding = await getEmbedding(chunk.content);
        await client.query(
          `INSERT INTO code_chunks (file_id, chunk_name, chunk_type, content, start_line, end_line, embedding) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
          [fileId, chunk.metadata.name, chunk.metadata.type, chunk.content, chunk.metadata.start_line, chunk.metadata.end_line, pgvector.toSql(chunkEmbedding)]
        );
      }
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      logger(`      Failed to process ${relativePath}: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
  logger(`[4/4] File sync complete. Processed ${processedCount} new or changed files.`);
}

// --- STAGE 2: Sync Git Commit History ---

// REFACTORED: This function now uses the imported prompt
async function generateTaskFromCommit(client: Client, projectId: number, commit: DefaultLogFields, git: SimpleGit, logger: IngestionLogger) {
    const diff = await git.show(['--patch', '--first-parent', commit.hash]);
    
    if (!diff || diff.trim().length < 50) { 
        logger(`      -> Commit ${commit.hash.substring(0,7)} is trivial, skipping task generation.`);
        return;
    }

    const prompt = generateTaskFromCommitPrompt(commit.message, diff);

    try {
        const response = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: [{ role: 'user', content: prompt }],
            response_format: { type: 'json_object' },
            max_tokens: 400, // Increased token limit for more detailed descriptions
            temperature: 0.1,
        });
        
        const responseText = response.choices[0].message.content?.trim();

        if (!responseText || responseText.toUpperCase() === 'NULL') {
            logger(`      -> AI determined commit is trivial, skipping task generation.`);
            return;
        }

        // MODIFIED: Destructure the new 'description' field
        const { title, category, description } = JSON.parse(responseText);

        if (!title || !category || !description) {
            throw new Error('AI response was missing title, category, or description.');
        }

        logger(`      -> AI generated task: [${category}] "${title}"`);

        // MODIFIED: The content to embed now includes the more detailed description
        const contentToEmbed = `[${category}] ${title}\n\n${description}\n\nCompleted in commit: ${commit.hash}`;
        const taskEmbedding = await getEmbedding(contentToEmbed);

        await client.query(
            `INSERT INTO tasks (project_id, title, description, status, category, embedding, created_at, updated_at) 
             VALUES ($1, $2, $3, 'done', $4, $5, $6, $6)`,
            [
                projectId, 
                title, 
                // MODIFIED: Use the AI-generated description
                description,
                category,
                pgvector.toSql(taskEmbedding),
                commit.date
            ]
        );
        logger(`      ‚úÖ Created and closed retrospective task for commit ${commit.hash.substring(0,7)}.`);

    } catch (error) {
        logger(`      ‚ùå Failed to generate task for commit ${commit.hash.substring(0,7)}: ${error instanceof Error ? error.message : 'Unknown AI or parsing error'}`);
    }
}

// This function now contains the core orchestration logic for git history.
async function syncGitHistory(client: Client, projectId: number, git: SimpleGit, logger: IngestionLogger) {
    logger('\n[1/3] Starting Git history sync...');
    
    const { rows: existingCommits } = await client.query('SELECT commit_hash FROM commits WHERE project_id = $1', [projectId]);
    const existingHashes = new Set(existingCommits.map(c => c.commit_hash));
    logger(`[2/3] Found ${existingHashes.size} existing commits in the database.`);

    const log: LogResult<DefaultLogFields> = await git.log();
    const allCommits = [...log.all].reverse();

    const newCommits = allCommits.filter(c => !existingHashes.has(c.hash));
    if (newCommits.length === 0) {
        logger('[3/3] Git history is already up-to-date.');
        return;
    }
    logger(`      Found ${newCommits.length} new commits to process.`);

    for (const commit of newCommits) {
        logger(`      Processing commit ${commit.hash.substring(0, 7)}: ${commit.message}`);
        
        await client.query('BEGIN');
        try {
            const messageEmbedding = await getEmbedding(commit.message);
            const commitInsertResult = await client.query(
                `INSERT INTO commits (project_id, commit_hash, author_name, author_email, commit_date, message, embedding) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id`,
                [projectId, commit.hash, commit.author_name, commit.author_email, commit.date, commit.message, pgvector.toSql(messageEmbedding)]
            );
            const commitId = commitInsertResult.rows[0].id;
            
            const diffSummary = await git.show(['--name-status', '--pretty=format:', commit.hash]);
            const changedFiles = diffSummary.split('\n').filter(line => line.trim());

            for (const line of changedFiles) {
                const parts = line.split('\t');
                if (parts.length < 2) continue;
                const change_type = parts[0].trim();
                const file_path = parts[1].trim();
                
                const { rows } = await client.query('SELECT id FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, file_path]);

                if (rows.length > 0) {
                    const fileId = rows[0].id;
                    await client.query(
                        `INSERT INTO commit_files (commit_id, file_id, change_type) VALUES ($1, $2, $3)`,
                        [commitId, fileId, change_type]
                    );
                }
            }

            const taskRegex = /(?:closes|fixes|resolves)\s+#(\d+)/gi;
            const match = taskRegex.exec(commit.message);

            if (match) {
                const taskNumber = parseInt(match[1], 10);
                logger(`      -> Found reference to close task #${taskNumber}.`);
                const updateResult = await client.query(
                    `UPDATE tasks SET status = 'done', updated_at = NOW() WHERE project_id = $1 AND task_number = $2 AND status != 'done'`,
                    [projectId, taskNumber]
                );
                if (updateResult.rowCount && updateResult.rowCount > 0) {
                    logger(`      ‚úÖ Automatically closed task #${taskNumber}.`);
                }
            } else {
                await generateTaskFromCommit(client, projectId, commit, git, logger);
            }

            await client.query('COMMIT');
        } catch (error) {
            await client.query('ROLLBACK');
            logger(`      Failed to process commit ${commit.hash}: ${error instanceof Error ? error.message : String(error)}`);
        }
    }
    logger('[3/3] Git history sync complete.');
}

--- FILE: api/tasks/task.controller.ts ---

// --- FILE: api/tasks/task.controller.ts ---

// src/api/tasks/task.controller.ts
import { Request, Response, NextFunction } from 'express';
import * as taskService from './task.service';

export async function listTasks(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const status = req.query.status as string | undefined; // Allow undefined
        const tasks = await taskService.getTasks(projectId, status);
        res.json(tasks);
    } catch (error) {
        next(error);
    }
}

// MODIFIED: Handle 'description' field from the body
export async function createTask(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const { title, description } = req.body; // <-- Get description here
        if (!title) {
            return res.status(400).json({ error: 'A "title" is required.' });
        }
        const newTask = await taskService.createTask(projectId, title, description); // <-- Pass it here
        res.status(201).json(newTask);
    } catch (error) {
        next(error);
    }
}

// MODIFIED: Handle multiple fields for update
export async function updateTask(req: Request, res: Response, next: NextFunction) {
    try {
        const { projectId, taskNumber } = req.params;
        const { title, description, status } = req.body; // <-- Get all potential updates
        
        const updatedTask = await taskService.updateTask(
            parseInt(projectId, 10),
            parseInt(taskNumber, 10),
            { title, description, status } // <-- Pass as an object
        );
        res.json(updatedTask);
    } catch (error) {
        next(error);
    }
}

--- FILE: api/tasks/task.routes.ts ---

// src/api/tasks/task.routes.ts
import { Router } from 'express';
import * as taskController from './task.controller';

const router = Router({ mergeParams: true }); // mergeParams is crucial for nested routes

router.get('/', taskController.listTasks);
router.post('/', taskController.createTask); 
router.put('/:taskNumber', taskController.updateTask); 

export default router;

--- FILE: api/tasks/task.service.ts ---

// src/api/tasks/task.service.ts
import pool from '../../services/db';
import { getEmbedding } from '../../services/openai';
import pgvector from 'pgvector/pg';

export async function getTasks(projectId: number, status?: string) {
    const client = await pool.connect();
    try {
        let query = 'SELECT * FROM tasks WHERE project_id = $1';
        const params: any[] = [projectId];
        if (status) {
            query += ' AND status = $2';
            params.push(status);
        }
        query += ' ORDER BY task_number ASC';
        const { rows } = await client.query(query, params);
        return rows;
    } finally {
        client.release();
    }
}

export async function createTask(projectId: number, title: string, description?: string) {
    const client = await pool.connect();
    try {
        const contentToEmbed = `${title}${description ? `\n\n${description}` : ''}`;
        const titleEmbedding = await getEmbedding(contentToEmbed);
        const { rows } = await client.query(
            'INSERT INTO tasks (project_id, title, description, embedding) VALUES ($1, $2, $3, $4) RETURNING *',
            [projectId, title, description || null, pgvector.toSql(titleEmbedding)]
        );
        return rows[0];
    } catch (error) {
        console.error('Task creation failed. Ensure the `tasks` table has `description` and `embedding` columns.');
        throw error;
    }
    finally {
        client.release();
    }
}

interface TaskUpdates {
    title?: string;
    description?: string;
    status?: 'open' | 'in_progress' | 'done';
}

export async function updateTask(projectId: number, taskNumber: number, updates: TaskUpdates) {
    const client = await pool.connect();
    try {
        const { rows: existingTasks } = await client.query(
            'SELECT title, description FROM tasks WHERE project_id = $1 AND task_number = $2',
            [projectId, taskNumber]
        );

        if (existingTasks.length === 0) {
            throw new Error('Task not found');
        }

        const currentTask = existingTasks[0];
        const newTitle = updates.title || currentTask.title;
        const newDescription = updates.description || currentTask.description;

        const fieldsToUpdate = [];
        const values = [];
        let paramIndex = 1;

        if (updates.status) {
            if (!['open', 'in_progress', 'done'].includes(updates.status)) {
                throw new Error('Invalid task status');
            }
            fieldsToUpdate.push(`status = $${paramIndex++}`);
            values.push(updates.status);
        }
        if (updates.title) {
            fieldsToUpdate.push(`title = $${paramIndex++}`);
            values.push(updates.title);
        }
        if (updates.description) {
            fieldsToUpdate.push(`description = $${paramIndex++}`);
            values.push(updates.description);
        }

        if (updates.title || updates.description) {
            const contentToEmbed = `${newTitle}${newDescription ? `\n\n${newDescription}` : ''}`;
            const newEmbedding = await getEmbedding(contentToEmbed);
            fieldsToUpdate.push(`embedding = $${paramIndex++}`);
            values.push(pgvector.toSql(newEmbedding));
        }

        if (fieldsToUpdate.length === 0) {
            return currentTask;
        }

        fieldsToUpdate.push(`updated_at = NOW()`);
        
        values.push(projectId, taskNumber);

        const query = `UPDATE tasks SET ${fieldsToUpdate.join(', ')} WHERE project_id = $${paramIndex++} AND task_number = $${paramIndex++} RETURNING *`;
        
        const { rows } = await client.query(query, values);

        return rows[0];
    } finally {
        client.release();
    }
}

--- FILE: api/projects/project.controller.ts ---

// src/api/projects/project.controller.ts
import { Request, Response, NextFunction } from 'express';
import * as projectService from './project.service';
import * as qaService from './qa.service';
import ingestionQueue from '../../services/queue.service';
import { UnsupportedFileTypeError } from '../../core/documentExtractor'; // <-- IMPORT THE CUSTOM ERROR


export async function listProjects(req: Request, res: Response, next: NextFunction) {
    try {
        const projects = await projectService.getAllProjects();
        res.json(projects);
    } catch (error) {
        next(error);
    }
}

export async function addProject(req: Request, res: Response, next: NextFunction) {
    try {
        const { source } = req.body;
        if (!source) {
            return res.status(400).json({ error: 'A "source" Git URL is required.' });
        }
        
        const { project, created } = await projectService.createProject(source);

        if (!created) {
            return res.status(200).json({ message: 'Project already exists.', project });
        }
        
        // Respond immediately and start ingestion in the background
        res.status(202).json({ message: 'Project created. Ingestion will start in the background.', project });
        projectService.startProjectIngestionInBackground(project.id, project.source);

    } catch (error) {
        next(error);
    }
}

// REMOVED old syncProject, which is replaced by streamIngestionLogs

// NEW: Controller for streaming ingestion logs
export async function streamIngestionLogs(req: Request, res: Response, next: NextFunction) {
    const projectId = parseInt(req.params.projectId, 10);
    
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.flushHeaders();

    const logger = (message: string) => {
        if (!res.writableEnded) {
            res.write(`data: ${JSON.stringify({ log: message })}\n\n`);
        }
    };

    req.on('close', () => {
        console.log(`Client disconnected from ingestion stream for project ${projectId}.`);
        res.end();
    });

    // MODIFIED: Wrap the entire ingestion logic in the queue
    ingestionQueue.add(async () => {
        try {
            const project = await projectService.getProjectById(projectId);
            if (!project) {
                logger(`Error: Project with ID ${projectId} not found.`);
                return; // Return from the job, not the outer function
            }
            
            logger('Your sync request is now being processed...');
            await projectService.startProjectIngestion(projectId, project.source, logger);
            
            logger('Ingestion complete.');
            res.write('event: end\ndata: {"message": "Ingestion complete"}\n\n');

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            logger(`FATAL ERROR: ${errorMessage}`);
            console.error("Error during ingestion stream:", error);
            res.write(`event: error\ndata: {"message": "${errorMessage}"}\n\n`);
        } finally {
            if (!res.writableEnded) {
                res.end();
            }
        }
    }).catch((err:any) => {
        // This catch is for errors adding the job to the queue itself, which is rare.
        console.error("Failed to add ingestion job to queue:", err);
        if (!res.writableEnded) {
            res.status(500).send("Failed to queue the ingestion job.");
        }
    });
}


export async function askQuestion(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const { question } = req.body;
        if (!question) {
            return res.status(400).json({ error: 'A "question" is required.' });
        }
        
        const stream = await qaService.getAnswerStream(projectId, question);
        
        res.setHeader('Content-Type', 'text/plain; charset=utf-8');
        for await (const chunk of stream) {
            res.write(chunk.choices[0]?.delta?.content || '');
        }
        res.end();

    } catch (error) {
        if (!res.headersSent) {
          next(error);
        } else {
          console.error("Error during streaming:", error);
          res.end();
        }
    }
}

export async function uploadDocument(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        if (!req.file) {
            return res.status(400).json({ error: 'No file uploaded.' });
        }
        
        const document = await projectService.addProjectDocument(
            projectId,
            req.file.originalname,
            req.file.path
        );

        res.status(201).json({ message: 'Document uploaded and indexed successfully.', document });
    } catch (error) {
        // MODIFIED: Catch specific error for unsupported file types
        if (error instanceof UnsupportedFileTypeError) {
            return res.status(400).json({ error: error.message });
        }
        next(error);
    }
}

// NEW: Controller to list all documents for a project
export async function listDocuments(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const documents = await projectService.getProjectDocuments(projectId);
        res.json(documents);
    } catch (error) {
        next(error);
    }
}

// NEW: Controller to delete a document
export async function deleteDocument(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const documentId = parseInt(req.params.documentId, 10);
        await projectService.deleteProjectDocument(projectId, documentId);
        res.status(204).send(); // 204 No Content is standard for successful deletions
    } catch (error) {
        next(error);
    }
}


// NEW: Controller for project stats
export async function getProjectStats(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const stats = await projectService.getProjectStats(projectId);
        res.json(stats);
    } catch (error) {
        next(error);
    }
}

--- FILE: api/projects/project.routes.ts ---

// src/api/projects/project.routes.ts
import { Router } from 'express';
import * as projectController from './project.controller';
import taskRoutes from '../tasks/task.routes';
import multer from 'multer';

const router = Router();
const upload = multer({ dest: 'uploads/' });

router.get('/', projectController.listProjects);
router.post('/', projectController.addProject);

// MODIFIED: This is now a GET request to establish an SSE connection for logs
router.get('/:projectId/sync-stream', projectController.streamIngestionLogs);

// NEW: Route to get project statistics
router.get('/:projectId/stats', projectController.getProjectStats);

router.post('/:projectId/ask', projectController.askQuestion);

// --- Document Routes ---
router.post('/:projectId/documents', upload.single('document'), projectController.uploadDocument);
// NEW: Route to list all documents for a project
router.get('/:projectId/documents', projectController.listDocuments);
// NEW: Route to delete a specific document
router.delete('/:projectId/documents/:documentId', projectController.deleteDocument);


// Mount task routes nested under projects
router.use('/:projectId/tasks', taskRoutes);

export default router;

--- FILE: api/projects/project.service.ts ---

// src/api/projects/project.service.ts
import pool from '../../services/db';
import { cloneOrPullRepo } from '../../services/git';
import { runIngestion, IngestionLogger } from '../../scripts/ingest';
import path from 'path';
import { chunkText } from '../../core/textChunker';
import { getEmbedding } from '../../services/openai';
import fs from 'fs/promises';
import pgvector from 'pgvector/pg';
import { extractTextFromFile } from '../../core/documentExtractor';

export async function getAllProjects() {
    const client = await pool.connect();
    try {
        const { rows } = await client.query('SELECT id, name, source FROM projects ORDER BY created_at DESC');
        return rows;
    } finally {
        client.release();
    }
}

export async function getProjectById(projectId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query('SELECT * FROM projects WHERE id = $1', [projectId]);
        if (rows.length === 0) {
            return null;
        }
        return rows[0];
    } finally {
        client.release();
    }
}


export async function createProject(source: string) {
    const client = await pool.connect();
    try {
        const existing = await client.query('SELECT * FROM projects WHERE source = $1', [source]);
        if (existing.rows.length > 0) {
            return { project: existing.rows[0], created: false };
        }

        const projectName = path.basename(source, path.extname(source));
        const { rows } = await client.query(
            'INSERT INTO projects (name, source) VALUES ($1, $2) RETURNING *',
            [projectName, source]
        );
        return { project: rows[0], created: true };
    } finally {
        client.release();
    }
}

export async function startProjectIngestion(projectId: number, source: string, logger: IngestionLogger) {
    try {
        const projectPath = await cloneOrPullRepo(source, logger);
        logger(`[Project ${projectId}] Ingestion running...`);
        await runIngestion(projectId, projectPath, logger);
        logger(`‚úÖ [Project ${projectId}] Ingestion complete.`);
    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);
        logger(`‚ùå [Project ${projectId}] Ingestion failed: ${errorMessage}`);
        console.error(`‚ùå [Project ${projectId}] Ingestion failed:`, error);
    }
}

export function startProjectIngestionInBackground(projectId: number, source: string) {
    startProjectIngestion(projectId, source, console.log);
}

export async function addProjectDocument(projectId: number, originalFilename: string, storedFilePath: string) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');
        
        const content = await extractTextFromFile(storedFilePath, originalFilename);
        
        const docResult = await client.query(
            'INSERT INTO project_documents (project_id, file_name, file_path) VALUES ($1, $2, $3) RETURNING id',
            [projectId, originalFilename, storedFilePath]
        );
        const documentId = docResult.rows[0].id;
        
        const chunks = chunkText(content);
        
        for (const chunk of chunks) {
            const embedding = await getEmbedding(chunk);
            await client.query(
                'INSERT INTO document_chunks (document_id, content, embedding) VALUES ($1, $2, $3)',
                [documentId, chunk, pgvector.toSql(embedding)]
            );
        }
        
        await client.query('COMMIT');
        return { id: documentId, file_name: originalFilename, file_path: storedFilePath };
    } catch (error) {
        await client.query('ROLLBACK');
        console.error(`Failed to process document ${originalFilename}:`, error);
        throw error;
    } finally {
        client.release();
    }
}

export async function getProjectDocuments(projectId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query(
            'SELECT id, file_name, created_at FROM project_documents WHERE project_id = $1 ORDER BY created_at DESC',
            [projectId]
        );
        return rows;
    } finally {
        client.release();
    }
}

export async function deleteProjectDocument(projectId: number, documentId: number) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');
        
        const docResult = await client.query(
            'SELECT file_path FROM project_documents WHERE id = $1 AND project_id = $2',
            [documentId, projectId]
        );

        if (docResult.rows.length === 0) {
            throw new Error('Document not found or does not belong to this project.');
        }
        const filePath = docResult.rows[0].file_path;
        
        await client.query('DELETE FROM document_chunks WHERE document_id = $1', [documentId]);
        await client.query('DELETE FROM project_documents WHERE id = $1', [documentId]);
        
        await client.query('COMMIT');

        if (filePath) {
            try {
                await fs.unlink(filePath);
            } catch (fileError) {
                console.error(`Failed to delete document file ${filePath}:`, fileError);
            }
        }
    } catch (error) {
        await client.query('ROLLBACK');
        console.error(`Failed to delete document ${documentId}:`, error);
        throw error;
    } finally {
        client.release();
    }
}

export async function getProjectStats(projectId: number) {
    const client = await pool.connect();
    try {
        const [
            fileStatsRes,
            taskStatsRes,
            docStatsRes,
            commitHistoryRes,
            contributorRes
        ] = await Promise.all([
            client.query(
                `SELECT
                    (SELECT COUNT(*) FROM indexed_files WHERE project_id = $1) as file_count,
                    (SELECT COUNT(*) FROM code_chunks WHERE file_id IN (SELECT id FROM indexed_files WHERE project_id = $1)) as chunk_count`,
                [projectId]
            ),
            client.query(
                `SELECT status, COUNT(*) as count FROM tasks WHERE project_id = $1 GROUP BY status`,
                [projectId]
            ),
            client.query(
                `SELECT COUNT(*) as document_count FROM project_documents WHERE project_id = $1`,
                [projectId]
            ),
            client.query(
                `SELECT commit_hash, author_name, commit_date, message FROM commits WHERE project_id = $1 ORDER BY commit_date DESC LIMIT 50`,
                [projectId]
            ),
            client.query(
                `SELECT COUNT(DISTINCT author_name) as contributor_count FROM commits WHERE project_id = $1`,
                [projectId]
            )
        ]);
        
        const taskStats = taskStatsRes.rows.reduce((acc, row) => {
            acc[row.status] = parseInt(row.count, 10);
            return acc;
        }, { open: 0, in_progress: 0, done: 0 });

        return {
            files: {
                count: parseInt(fileStatsRes.rows[0].file_count, 10),
                chunks: parseInt(fileStatsRes.rows[0].chunk_count, 10),
            },
            tasks: taskStats,
            documents: {
                count: parseInt(docStatsRes.rows[0].document_count, 10),
            },
            git: {
                commitCount: commitHistoryRes.rows.length,
                contributorCount: parseInt(contributorRes.rows[0].contributor_count, 10),
                history: commitHistoryRes.rows,
            }
        };
    } finally {
        client.release();
    }
}

--- FILE: api/projects/qa.service.ts ---

// src/api/projects/qa.service.ts
import pool from '../../services/db';
import * as openAI from '../../services/openai';
import pgvector from 'pgvector/pg';
import { PoolClient } from 'pg';

export async function getAnswerStream(projectId: number, question: string) {
    let client: PoolClient | null = null;
    try {
        client = await pool.connect();
        const questionEmbedding = await openAI.getEmbedding(question);
        let contextString = '';

        const { rows: relevantTasks } = await client.query(
            `SELECT task_number, title, status FROM tasks WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        if (relevantTasks.length > 0) {
            contextString += "Relevant Tasks:\n" + relevantTasks.map(t => `- Task #${t.task_number} [${t.status.toUpperCase()}]: ${t.title}`).join('\n') + '\n\n';
        }

        const { rows: relevantCommits } = await client.query(
            `SELECT commit_hash, message, author_name FROM commits WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        if (relevantCommits.length > 0) {
            contextString += "Relevant Commits:\n" + relevantCommits.map(c => `- Commit ${c.commit_hash.substring(0, 7)} by ${c.author_name}: ${c.message.split('\n')[0]}`).join('\n') + '\n\n';
        }
        
        try {
            const { rows: relevantDocChunks } = await client.query(
                `SELECT
                   pd.file_name,
                   dc.content
                 FROM document_chunks dc
                 JOIN project_documents pd ON dc.document_id = pd.id
                 WHERE pd.project_id = $1
                 ORDER BY dc.embedding <=> $2
                 LIMIT 3`,
                [projectId, pgvector.toSql(questionEmbedding)]
            );
            if (relevantDocChunks.length > 0) {
                contextString += "Relevant Project Documents:\n" + relevantDocChunks.map(d => `--- FROM DOCUMENT: ${d.file_name} ---\n\n${d.content}`).join('\n\n') + '\n\n';
            }
        } catch (error: any) {
            if (error.code === '42P01') {
                 console.warn('Warning: project_documents or document_chunks table not found. Skipping document search. Run migrations to enable this feature.');
            } else {
                throw error;
            }
        }

        const { rows: relevantFiles } = await client.query(
            `SELECT id, path FROM indexed_files WHERE project_id = $1 ORDER BY summary_embedding <=> $2 LIMIT 5`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        
        if (relevantFiles.length > 0) {
            const relevantFileIds = relevantFiles.map(f => f.id);
            const { rows: contextChunks } = await client.query(
                `SELECT file_id, content, chunk_name FROM code_chunks WHERE file_id = ANY($1::int[]) ORDER BY embedding <=> $2 LIMIT 10`,
                [relevantFileIds, pgvector.toSql(questionEmbedding)]
            );
            
            if (contextChunks.length > 0) {
                 const chunkContext = contextChunks.map(c => {
                    const filePath = relevantFiles.find(f => f.id === c.file_id)?.path;
                    return `--- FILE: ${filePath} (Chunk: ${c.chunk_name}) ---\n\n${c.content}`;
                }).join('\n\n');
                contextString += "Relevant Code Snippets:\n" + chunkContext;
            }
        }
        
        if (!contextString.trim()) {
            throw new Error("No relevant context found for this question (no tasks, commits, or code).");
        }

        const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided context, which may include project documents, tasks, commits, and code snippets. Be concise, accurate, and provide code snippets in Markdown format when relevant. If the context is insufficient, state that clearly.`;
        const userPrompt = `CONTEXT:\n${contextString}\n\nQUESTION:\n${question}`;
        
        // Release the client back to the pool before we return the stream
        client.release();
        client = null; // Prevent the finally block from releasing it again

        return openAI.getChatCompletionStream(systemPrompt, userPrompt);

    } finally {
        // Ensure the client is always released if it exists
        if (client) {
            client.release();
        }
    }
}

--- FILE: services/db.ts ---

// src/services/db.ts
import { Pool } from 'pg';
import pgvector from 'pgvector/pg';

const connectionString = process.env.DATABASE_URL!;

if (!connectionString) {
    throw new Error("FATAL: Missing environment variable DATABASE_URL");
}

// Create a single, shared pool for the entire application
const pool = new Pool({ connectionString });

// Add a listener to register the vector type on each new connection
// that the pool creates.
pool.on('connect', async (client) => {
    await pgvector.registerType(client);
});

export default pool;

--- FILE: services/git.ts ---

// src/services/git.ts
import simpleGit from 'simple-git';
import { promises as fs } from 'fs';
import path from 'path';
import os from 'os';

const WORKSPACE_DIR = path.join(os.homedir(), '.ai-brain-workspace');
// Optional: Define a logger type for clarity
type GitLogger = (message: string) => void;


export function getWorkspacePathFromUrl(url: string): string {
    try {
        const parsedUrl = new URL(url);
        const cleanPath = (parsedUrl.hostname + parsedUrl.pathname).replace(/\.git$/, '');
        return path.join(WORKSPACE_DIR, cleanPath);
    } catch (e) {
        const sshMatch = url.match(/git@([^:]+):(.*)/);
        if (sshMatch) {
            const host = sshMatch[1];
            const repoPath = sshMatch[2].replace(/\.git$/, '');
            return path.join(WORKSPACE_DIR, host, repoPath);
        }
        return path.join(WORKSPACE_DIR, url.replace(/[^a-zA-Z0-9]/g, '_'));
    }
}

export async function cloneOrPullRepo(source: string, logger: GitLogger = console.log): Promise<string> {
    const projectPath = getWorkspacePathFromUrl(source);
    await fs.mkdir(WORKSPACE_DIR, { recursive: true });

    try {
        await fs.access(path.join(projectPath, '.git'));
        logger(`Found existing repository. Fetching updates from ${source}...`);
        await simpleGit(projectPath).pull();
        logger(`-> Updates pulled successfully.`);
    } catch (error) {
        logger(`Cloning repository from ${source}...`);
        await simpleGit().clone(source, projectPath);
        logger(`-> Cloned successfully.`);
    }
    return projectPath;
}

--- FILE: services/openai.ts ---

// src/services/openai.ts
import OpenAI from 'openai';

const openaiApiKey = process.env.OPENAI_API_KEY!;
if (!openaiApiKey) {
    throw new Error("FATAL: Missing environment variable OPENAI_API_KEY");
}

const openai = new OpenAI({ apiKey: openaiApiKey });

export async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

export async function getChatCompletionStream(systemPrompt: string, userPrompt: string) {
    return openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
        stream: true,
    });
}

--- FILE: services/queue.service.ts ---

// --- FILE: services/queue.service.ts ---
import PQueue from 'p-queue';

// Create a single, shared queue for the entire application.
// concurrency: 1 ensures that only one promise runs at a time.
// This is our lock to prevent multiple ingestions from running simultaneously.
const ingestionQueue = new PQueue({ concurrency: 1 });

export default ingestionQueue;