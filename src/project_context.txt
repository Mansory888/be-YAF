Project file structure:
=======================
./
    brain.ts
    server.ts
    core/
        chunker.ts
    scripts/
        ingest.ts


File Contents:
===============


--- FILE: brain.ts ---

#!/usr/bin/env node
// --- FILE: brain.ts ---

import 'dotenv/config';
import { Command } from 'commander';
import { Client } from 'pg';
import pgvector from 'pgvector/pg';
import OpenAI from 'openai';
import { runIngestion } from './scripts/ingest';
import simpleGit, { SimpleGit } from 'simple-git';
import { promises as fs } from 'fs';
import path from 'path';
import os from 'os';

// --- CONFIGURATION ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;
// NEW: Define a persistent workspace directory in the user's home folder
const WORKSPACE_DIR = path.join(os.homedir(), '.ai-brain-workspace');

if (!connectionString || !openaiApiKey) {
    throw new Error("FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY");
}
const openai = new OpenAI({ apiKey: openaiApiKey });
const program = new Command();

// --- HELPER FUNCTIONS ---
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

/**
 * Creates a stable, filesystem-safe directory path from a Git URL.
 * This ensures the same URL always maps to the same local folder.
 * e.g., https://github.com/foo/bar.git -> ~/.ai-brain-workspace/github.com/foo/bar
 */
function getWorkspacePathFromUrl(url: string): string {
    try {
        const parsedUrl = new URL(url);
        // a.com/b/c.git -> a.com/b/c
        const cleanPath = (parsedUrl.hostname + parsedUrl.pathname).replace(/\.git$/, '');
        return path.join(WORKSPACE_DIR, cleanPath);
    } catch (e) {
        // Handle SSH URLs like git@github.com:foo/bar.git
        const sshMatch = url.match(/git@([^:]+):(.*)/);
        if (sshMatch) {
            const host = sshMatch[1];
            const repoPath = sshMatch[2].replace(/\.git$/, '');
            return path.join(WORKSPACE_DIR, host, repoPath);
        }
        // Fallback for other formats by replacing non-alphanumeric chars
        return path.join(WORKSPACE_DIR, url.replace(/[^a-zA-Z0-9]/g, '_'));
    }
}

// --- CLI COMMAND DEFINITIONS ---

program
  .command('ingest')
  .description('Ingest a project from a local path or a public Git URL.')
  .argument('<source>', 'The local path or Git URL of the project')
  .action(async (source: string) => {
    let projectPath = source;

    // If the source is a URL, manage it in the persistent workspace
    if (source.startsWith('http') || source.startsWith('git@')) {
        projectPath = getWorkspacePathFromUrl(source);
        
        try {
            // Ensure the base workspace directory exists
            await fs.mkdir(WORKSPACE_DIR, { recursive: true });

            // Check if the repo has already been cloned
            try {
                await fs.access(path.join(projectPath, '.git')); // Check for .git dir
                // It exists, so pull the latest changes
                console.log(`üß† Found existing repository. Fetching updates from ${source}...`);
                const git: SimpleGit = simpleGit(projectPath);
                await git.pull();
                console.log(`   -> Updates pulled successfully.`);
            } catch (error) {
                // It doesn't exist, so clone it
                console.log(`üß† Cloning repository from ${source}...`);
                await simpleGit().clone(source, projectPath);
                console.log(`   -> Cloned successfully into: ${projectPath}`);
            }
        } catch (gitError) {
            console.error('‚ùå A Git error occurred:', gitError);
            process.exit(1);
        }
    }

    try {
      await runIngestion(projectPath);
      console.log('‚úÖ Ingestion complete.');
    } catch (error) {
      console.error('‚ùå Ingestion failed:', error);
      process.exit(1);
    }
  });

// The `ask` command (remains unchanged)
program
  .command('ask')
  .description('Ask a question about the indexed codebase.')
  .argument('<question>', 'The question to ask')
  .action(async (question: string) => {
    console.log(`üß† Thinking about: "${question}"`);
    const client = new Client({ connectionString });
    try {
      await client.connect();
      await pgvector.registerType(client);

      const questionEmbedding = await getEmbedding(question);

      const { rows: relevantFiles } = await client.query(
        `SELECT id, path, summary FROM indexed_files ORDER BY summary_embedding <=> $1 LIMIT 5`,
        [pgvector.toSql(questionEmbedding)]
      );
      
      if (relevantFiles.length === 0) {
        console.log("I couldn't find any relevant files to answer that question.");
        return;
      }
      
      console.log(`\nüîç Found relevant files: ${relevantFiles.map(f => f.path).join(', ')}`);
      const relevantFileIds = relevantFiles.map(f => f.id);

      const { rows: contextChunks } = await client.query(
        `SELECT file_id, content, chunk_name 
         FROM code_chunks 
         WHERE file_id = ANY($1::int[])
         ORDER BY embedding <=> $2 
         LIMIT 10`,
        [relevantFileIds, pgvector.toSql(questionEmbedding)]
      );

      if (contextChunks.length === 0) {
        console.log("I found some relevant files, but couldn't pinpoint specific code snippets to answer your question.");
        return;
      }

      const contextString = contextChunks.map(c => {
        const filePath = relevantFiles.find(f => f.id === c.file_id)?.path;
        return `--- FILE: ${filePath} (Chunk: ${c.chunk_name}) ---\n\n${c.content}`;
      }).join('\n\n');

      const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided code context. Be concise, accurate, and provide code snippets in Markdown format when relevant. If the context is insufficient, state that clearly.`;
      const userPrompt = `CONTEXT:\n${contextString}\n\nQUESTION:\n${question}`;

      const stream = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
        stream: true,
      });
      
      console.log('\nüí¨ Answer:\n');
      for await (const chunk of stream) {
        process.stdout.write(chunk.choices[0]?.delta?.content || '');
      }
      console.log('\n');

    } catch (error) {
      console.error('‚ùå An error occurred:', error);
    } finally {
      await client.end();
    }
  });

program.parse(process.argv);

--- FILE: server.ts ---

import 'dotenv/config';
import express from 'express';
import path from 'path';
import { Client } from 'pg';
import OpenAI from 'openai';
import pgvector from 'pgvector/pg'; // ‚úÖ correct import

// --- CONFIGURATION & VALIDATION ---
const app = express();
const port = 3000;
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;

if (!connectionString || !openaiApiKey) {
    throw new Error("FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY");
}
const openai = new OpenAI({ apiKey: openaiApiKey });

// --- MIDDLEWARE ---
app.use(express.json());
app.use(express.static(path.join(process.cwd(), 'public')));

// --- HELPER FUNCTION ---
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

// --- API ENDPOINT ---
app.post('/api/ask', async (req, res) => {
    const { question } = req.body;
    if (!question) {
        return res.status(400).json({ error: 'Question is required.' });
    }
    console.log(`Received question: ${question}`);

    const client = new Client({ connectionString });
    try {
        await client.connect();

        // ‚úÖ REGISTER THE VECTOR TYPE FOR THE QUERY
        await pgvector.registerType(client);
        
        const questionEmbedding = await getEmbedding(question);

        console.log('Querying for similar chunks...');
        const { rows: contextChunks } = await client.query(
            `SELECT file_path, content, embedding <=> $1 AS distance
             FROM code_chunks
             ORDER BY distance
             LIMIT 10`,
            [pgvector.toSql(questionEmbedding)] // ‚úÖ USE THE HELPER TO FORMAT THE VECTOR
        );
        
        console.log(`Found ${contextChunks.length} relevant chunks.`);
        if (contextChunks.length > 0) {
            console.log('Top result distance:', contextChunks[0].distance);
        }

        if (contextChunks.length === 0) {
            return res.json({ answer: "I couldn't find any relevant context in the codebase to answer that." });
        }

        const contextString = contextChunks.map(c => `--- FILE: ${c.file_path} ---\n\n${c.content}`).join('\n\n');
        const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided code context. Be concise and accurate. Format code blocks using Markdown. If the context is insufficient, say so.`;
        const userPrompt = `CONTEXT:\n${contextString}\n\nQUESTION:\n${question}`;

        const stream = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
            stream: true,
        });

        res.setHeader('Content-Type', 'text/plain; charset=utf-8');
        for await (const chunk of stream) {
            res.write(chunk.choices[0]?.delta?.content || '');
        }
        res.end();
    } catch (error) {
        console.error('Error in /api/ask:', error);
        res.status(500).json({ error: 'An internal server error occurred.' });
    } finally {
        // Ensure the client is always closed
        if (client) {
            await client.end();
        }
    }
});

// --- START SERVER ---
app.listen(port, () => {
    console.log(`üß† AI Project Brain is listening at http://localhost:${port}`);
});

--- FILE: core/chunker.ts ---

// --- FILE: core/chunker.ts ---
import Parser, { Query } from 'tree-sitter'; // MODIFIED: Import the Query class
import TypeScript from 'tree-sitter-typescript/typescript';

export interface CodeChunk {
  content: string;
  metadata: {
    type: 'function' | 'class' | 'method' | 'arrow_function' | 'block';
    name: string;
    start_line: number;
    end_line: number;
  };
}

const parser = new Parser();
parser.setLanguage(TypeScript);

// CRITICAL: This query identifies the distinct, semantic blocks of code.
const TS_QUERY = `
[
  (function_declaration) @chunk
  (class_declaration) @chunk
  (method_definition) @chunk
  (lexical_declaration 
    (variable_declarator 
      value: (arrow_function)
    )
  ) @chunk
]
`;

export function chunkCodeWithAST(content: string): CodeChunk[] {
  const tree = parser.parse(content);
  // MODIFIED: Use the Query constructor for robustness
  const query = new Query(TypeScript, TS_QUERY);
  const matches = query.captures(tree.rootNode);

  const chunks: CodeChunk[] = [];

  for (const match of matches) {
    const node = match.node;
    let name = 'anonymous';
    let type: CodeChunk['metadata']['type'] = 'block';

    // Heuristics to find the name of the chunk
    if (node.type === 'function_declaration' || node.type === 'class_declaration' || node.type === 'method_definition') {
        name = node.childForFieldName('name')?.text || 'anonymous';
        if (node.type === 'method_definition') type = 'method';
        else if (node.type === 'class_declaration') type = 'class';
        else type = 'function';
    } else if (node.type === 'lexical_declaration') {
        // For arrow functions like `const myFunc = () => ...`
        name = node.firstNamedChild?.firstNamedChild?.text || 'anonymous_arrow_function';
        type = 'arrow_function';
    }
    
    chunks.push({
      content: node.text,
      metadata: {
        type: type,
        name: name,
        start_line: node.startPosition.row + 1,
        end_line: node.endPosition.row + 1,
      },
    });
  }

  // Fallback: If no specific chunks were found (e.g., a simple config file),
  // treat the entire file as a single chunk.
  if (chunks.length === 0 && content.trim().length > 0) {
    chunks.push({
      content: content,
      metadata: {
        type: 'block',
        name: 'file_content',
        start_line: 1,
        end_line: content.split('\n').length,
      },
    });
  }

  return chunks;
}

--- FILE: scripts/ingest.ts ---

// --- FILE: scripts/ingest.ts ---

import 'dotenv/config';
import { glob } from 'glob';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { Client } from 'pg';
import OpenAI from 'openai';
import gitignore from 'gitignore-parser';
import pgvector from 'pgvector/pg';
import { chunkCodeWithAST } from '../core/chunker';
import simpleGit, { SimpleGit } from 'simple-git';

// --- CONFIGURATION (Global) ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;

if (!connectionString || !openaiApiKey) {
  throw new Error('FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY');
}

const openai = new OpenAI({ apiKey: openaiApiKey });
const IGNORED_EXTENSIONS = new Set(['.lock', '.svg', '.png', '.jpg', '.jpeg', '.gif', '.ico']);
const IGNORED_FILENAMES = new Set(['package-lock.json', 'yarn.lock', 'pnpm-lock.yaml']);

// --- CORE HELPER FUNCTIONS ---
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

async function summarizeFile(filePath: string, content: string): Promise<string> {
  const prompt = `Summarize the purpose of the following code file in one sentence. File Path: ${filePath}\n\nCode:\n---\n${content}\n---\n\nOne-sentence summary:`;
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 100,
      temperature: 0.1,
    });
    return response.choices[0].message.content?.trim() || "Could not generate a summary.";
  } catch (error) {
    console.error(`  - Failed to summarize ${filePath}:`, error);
    return "Summary generation failed.";
  }
}

// --- MAIN INGESTION LOGIC ---
export async function runIngestion(projectPath: string) {
  const client = new Client({ connectionString });
  await client.connect();
  await pgvector.registerType(client);
  console.log('Database connection established.');

  // FIX: Initialize simple-git here, using the provided projectPath.
  const git: SimpleGit = simpleGit(projectPath);

  try {
    // FIX: Pass the projectPath to syncFiles.
    await syncFiles(client, projectPath);
    // FIX: Pass the initialized git instance to syncGitHistory.
    await syncGitHistory(client, git);
  } finally {
    console.log('Ingestion complete. Closing database connection.');
    await client.end();
  }
}

// --- STAGE 1: Sync Filesystem State ---
// FIX: Update function signature to accept projectPath.
async function syncFiles(client: Client, projectPath: string) {
  console.log(`[1/4] Starting file sync for project: ${projectPath}`);
  
  // --- NEW: PRUNING STEP ---
  console.log(`[2/4] Pruning deleted files from the database...`);
  // Get all file paths currently in the database
  const { rows: dbFiles } = await client.query('SELECT path FROM indexed_files');
  const dbPaths = new Set(dbFiles.map(f => f.path));

  // Get all file paths currently on disk
  const allDiskFiles = await glob('**/*', { cwd: projectPath, nodir: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] });
  const diskPaths = new Set(allDiskFiles);

  // Find paths that are in the DB but NOT on disk
  const pathsToDelete = [...dbPaths].filter(p => !diskPaths.has(p));

  if (pathsToDelete.length > 0) {
    console.log(`      Found ${pathsToDelete.length} files to delete:`, pathsToDelete.join(', '));
    await client.query('DELETE FROM indexed_files WHERE path = ANY($1::text[])', [pathsToDelete]);
    console.log(`      -> Pruning complete.`);
  } else {
    console.log(`      -> No files to prune.`);
  }
  // --- END OF PRUNING STEP ---

  const gitignorePath = path.join(projectPath, '.gitignore');
  const ignore = fs.existsSync(gitignorePath)
    ? gitignore.compile(fs.readFileSync(gitignorePath, 'utf8'))
    : { accepts: (_p: string) => true };

  const filesToIndex = allDiskFiles.filter(file => { // Reuse the glob result
    const ext = path.extname(file);
    const filename = path.basename(file);
    return ignore.accepts(file) && !IGNORED_EXTENSIONS.has(ext) && !IGNORED_FILENAMES.has(filename);
  });

  console.log(`[3/4] Found ${filesToIndex.length} files to process for additions/modifications.`);
  let processedCount = 0;

  for (const relativePath of filesToIndex) {
    const fullPath = path.join(projectPath, relativePath);
    const content = fs.readFileSync(fullPath, 'utf-8');
    if (!content.trim()) continue;

    const hash = crypto.createHash('sha256').update(content).digest('hex');
    const { rows } = await client.query('SELECT content_hash FROM indexed_files WHERE path = $1', [relativePath]);

    if (rows.length > 0 && rows[0].content_hash === hash) {
      continue;
    }

    processedCount++;
    console.log(`      Processing changed file: ${relativePath}`);

    await client.query('BEGIN');
    try {
      await client.query('DELETE FROM indexed_files WHERE path = $1', [relativePath]);
      const summary = await summarizeFile(relativePath, content);
      const summaryEmbedding = await getEmbedding(summary);
      const fileInsertResult = await client.query(
        'INSERT INTO indexed_files (path, content_hash, summary, summary_embedding, last_indexed_at) VALUES ($1, $2, $3, $4, NOW()) RETURNING id',
        [relativePath, hash, summary, pgvector.toSql(summaryEmbedding)]
      );
      const fileId = fileInsertResult.rows[0].id;
      
      const chunks = chunkCodeWithAST(content);
      for (const chunk of chunks) {
        const chunkEmbedding = await getEmbedding(chunk.content);
        await client.query(
          `INSERT INTO code_chunks (file_id, chunk_name, chunk_type, content, start_line, end_line, embedding) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
          [fileId, chunk.metadata.name, chunk.metadata.type, chunk.content, chunk.metadata.start_line, chunk.metadata.end_line, pgvector.toSql(chunkEmbedding)]
        );
      }
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      console.error(`      Failed to process ${relativePath}:`, error);
    }
  }
  console.log(`[4/4] File sync complete. Processed ${processedCount} new or changed files.`);
}

// --- STAGE 2: Sync Git Commit History ---
// FIX: Update function signature to accept the git instance.
async function syncGitHistory(client: Client, git: SimpleGit) {
    console.log('\n[1/3] Starting Git history sync...');
    
    const { rows: existingCommits } = await client.query('SELECT commit_hash FROM commits');
    const existingHashes = new Set(existingCommits.map(c => c.commit_hash));
    console.log(`[2/3] Found ${existingHashes.size} existing commits in the database.`);

    // FIX: Use the passed git instance.
    const log = await git.log();
    const allCommits = [...log.all].reverse();

    const newCommits = allCommits.filter(c => !existingHashes.has(c.hash));
    if (newCommits.length === 0) {
        console.log('[3/3] Git history is already up-to-date.');
        return;
    }
    console.log(`      Found ${newCommits.length} new commits to process.`);

    for (const commit of newCommits) {
        console.log(`      Processing commit ${commit.hash.substring(0, 7)}: ${commit.message}`);
        
        await client.query('BEGIN');
        try {
            const messageEmbedding = await getEmbedding(commit.message);
            const commitInsertResult = await client.query(
                `INSERT INTO commits (commit_hash, author_name, author_email, commit_date, message, embedding) VALUES ($1, $2, $3, $4, $5, $6) RETURNING id`,
                [commit.hash, commit.author_name, commit.author_email, commit.date, commit.message, pgvector.toSql(messageEmbedding)]
            );
            const commitId = commitInsertResult.rows[0].id;
            
            const diff = await git.show(['--name-status', '--pretty=format:', commit.hash]);
            const changedFiles = diff.split('\n').filter(line => line.trim());

            for (const line of changedFiles) {
                const parts = line.split('\t');
                if (parts.length < 2) continue;
                const change_type = parts[0].trim();
                const file_path = parts[1].trim();
                
                const { rows } = await client.query('SELECT id FROM indexed_files WHERE path = $1', [file_path]);

                if (rows.length > 0) {
                    const fileId = rows[0].id;
                    await client.query(
                        `INSERT INTO commit_files (commit_id, file_id, change_type) VALUES ($1, $2, $3)`,
                        [commitId, fileId, change_type]
                    );
                }
            }
            await client.query('COMMIT');
        } catch (error) {
            await client.query('ROLLBACK');
            console.error(`      Failed to process commit ${commit.hash}:`, error);
        }
    }
    console.log('[3/3] Git history sync complete.');
}