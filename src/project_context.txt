Project file structure:
=======================
./
    app.ts
    brain.ts
    server.ts
    middleware/
        errorHandler.ts
    types/
        tree-sitter-typescript.d.ts
    core/
        chunker.ts
        documentExtractor.ts
        textChunker.ts
        prompts/
            fileSummary.prompt.ts
            taskGeneration.prompt.ts
    scripts/
        ingest.ts
    api/
        tasks/
            task.controller.ts
            task.routes.ts
            task.service.ts
        projects/
            project.controller.ts
            project.routes.ts
            project.service.ts
            qa.service.ts
        conversations/
            conversation.controller.ts
            conversation.routes.ts
            conversation.service.ts
    services/
        db.ts
        git.ts
        openai.ts
        queue.service.ts


File Contents:
===============


--- FILE: app.ts ---

// src/app.ts
import 'dotenv/config';
import express from 'express';
import path from 'path';
import projectRoutes from './api/projects/project.routes';
import cors from 'cors';
import { errorHandler } from './middleware/errorHandler';

const app = express();
app.use(cors());

// Middleware
app.use(express.json());
app.use(express.static(path.join(process.cwd(), 'public')));

// API Routes
app.use('/api/projects', projectRoutes);

// Error Handler
app.use(errorHandler);

export default app;

--- FILE: brain.ts ---

#!/usr/bin/env node
// --- FILE: brain.ts ---

import 'dotenv/config';
import { Command } from 'commander';
import { Client } from 'pg';
import pgvector from 'pgvector/pg';
import OpenAI from 'openai';
import { runIngestion } from './scripts/ingest';
import simpleGit, { SimpleGit } from 'simple-git';
import { promises as fs } from 'fs';
import path from 'path';
import os from 'os';

// --- CONFIGURATION ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;
const WORKSPACE_DIR = path.join(os.homedir(), '.ai-brain-workspace');

if (!connectionString || !openaiApiKey) {
    throw new Error("FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY");
}
const openai = new OpenAI({ apiKey: openaiApiKey });
const program = new Command();

// --- HELPER FUNCTIONS ---

/**
 * Retrieves the ID for a project from the database based on its source (Git URL or path).
 * If the project doesn't exist, it creates a new one.
 * @param source The unique Git URL or local path for the project.
 * @param client The active Postgres client.
 * @returns The numeric ID of the project.
 */
async function getProjectId(source: string, client: Client): Promise<number> {
    const projectRes = await client.query('SELECT id FROM projects WHERE source = $1', [source]);
    if (projectRes.rows.length > 0) {
        return projectRes.rows[0].id;
    } else {
        const projectName = path.basename(source, path.extname(source));
        console.log(`‚ú® Creating new project entry for '${projectName}'...`);
        const newProjectRes = await client.query(
            'INSERT INTO projects (name, source) VALUES ($1, $2) RETURNING id',
            [projectName, source]
        );
        return newProjectRes.rows[0].id;
    }
}

async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

function getWorkspacePathFromUrl(url: string): string {
    try {
        const parsedUrl = new URL(url);
        const cleanPath = (parsedUrl.hostname + parsedUrl.pathname).replace(/\.git$/, '');
        return path.join(WORKSPACE_DIR, cleanPath);
    } catch (e) {
        const sshMatch = url.match(/git@([^:]+):(.*)/);
        if (sshMatch) {
            const host = sshMatch[1];
            const repoPath = sshMatch[2].replace(/\.git$/, '');
            return path.join(WORKSPACE_DIR, host, repoPath);
        }
        return path.join(WORKSPACE_DIR, url.replace(/[^a-zA-Z0-9]/g, '_'));
    }
}

// --- CLI COMMAND DEFINITIONS ---

program
  .command('ingest')
  .description('Ingest a project from a local path or a public Git URL.')
  .argument('<source>', 'The local path or Git URL of the project')
  .action(async (source: string) => {
    let projectPath = source;

    if (source.startsWith('http') || source.startsWith('git@')) {
        projectPath = getWorkspacePathFromUrl(source);
        try {
            await fs.mkdir(WORKSPACE_DIR, { recursive: true });
            try {
                await fs.access(path.join(projectPath, '.git'));
                console.log(`üß† Found existing repository. Fetching updates from ${source}...`);
                await simpleGit(projectPath).pull();
                console.log(`   -> Updates pulled successfully.`);
            } catch (error) {
                console.log(`üß† Cloning repository from ${source}...`);
                await simpleGit().clone(source, projectPath);
                console.log(`   -> Cloned successfully into: ${projectPath}`);
            }
        } catch (gitError) {
            console.error('‚ùå A Git error occurred:', gitError);
            process.exit(1);
        }
    }

    const client = new Client({ connectionString });
    try {
      await client.connect();
      const projectId = await getProjectId(source, client);
      await runIngestion(projectId, projectPath, console.log);
      console.log('‚úÖ Ingestion complete.');
    } catch (error) {
      console.error('‚ùå Ingestion failed:', error);
      process.exit(1);
    } finally {
      await client.end();
    }
  });

program
  .command('ask')
  .description('Ask a question about the indexed codebase.')
  .argument('<question>', 'The question to ask')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .action(async (question: string, options: { project: string }) => {
    console.log(`üß† Thinking about: "${question}"`);
    const client = new Client({ connectionString });
    try {
      await client.connect();
      await pgvector.registerType(client);
      const projectId = await getProjectId(options.project, client);

      const questionEmbedding = await getEmbedding(question);
      let contextString = '';

      // --- Retrieve relevant tasks ---
      const { rows: relevantTasks } = await client.query(
        `SELECT task_number, title, status FROM tasks WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );

      if (relevantTasks.length > 0) {
        console.log(`\nüîç Found relevant tasks: ${relevantTasks.map(t => `#${t.task_number}`).join(', ')}`);
        contextString += "Relevant Tasks:\n" + relevantTasks.map(t => `- Task #${t.task_number} [${t.status.toUpperCase()}]: ${t.title}`).join('\n') + '\n\n';
      }

      // --- Retrieve relevant commits ---
      const { rows: relevantCommits } = await client.query(
        `SELECT commit_hash, message, author_name FROM commits WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );
      
      if (relevantCommits.length > 0) {
          console.log(`\nüîç Found relevant commits: ${relevantCommits.map(c => c.commit_hash.substring(0, 7)).join(', ')}`);
          contextString += "Relevant Commits:\n" + relevantCommits.map(c => `- Commit ${c.commit_hash.substring(0, 7)} by ${c.author_name}: ${c.message.split('\n')[0]}`).join('\n') + '\n\n';
      }

      // --- Retrieve relevant files and code chunks ---
      const { rows: relevantFiles } = await client.query(
        `SELECT id, path, summary FROM indexed_files WHERE project_id = $1 ORDER BY summary_embedding <=> $2 LIMIT 5`,
        [projectId, pgvector.toSql(questionEmbedding)]
      );
      
      if (relevantFiles.length === 0 && relevantTasks.length === 0 && relevantCommits.length === 0) {
        console.log("I couldn't find any relevant information to answer that question.");
        return;
      }
      
      if (relevantFiles.length > 0) {
        console.log(`\nüîç Found relevant files: ${relevantFiles.map(f => f.path).join(', ')}`);
        const relevantFileIds = relevantFiles.map(f => f.id);

        const { rows: contextChunks } = await client.query(
          `SELECT file_id, content, chunk_name FROM code_chunks WHERE file_id = ANY($1::int[]) ORDER BY embedding <=> $2 LIMIT 10`,
          [relevantFileIds, pgvector.toSql(questionEmbedding)]
        );

        if (contextChunks.length > 0) {
          const chunkContext = contextChunks.map(c => {
            const filePath = relevantFiles.find(f => f.id === c.file_id)?.path;
            return `--- FILE: ${filePath} (Chunk: ${c.chunk_name}) ---\n\n${c.content}`;
          }).join('\n\n');
          contextString += "Relevant Code Snippets:\n" + chunkContext;
        }
      }

      if (!contextString.trim()) {
        console.log("I found some potentially relevant items but couldn't construct a concrete context to answer your question.");
        return;
      }

      const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided context, which may include tasks, commits, and code snippets. Be concise, accurate, and provide code snippets in Markdown format when relevant. If the context is insufficient, state that clearly.`;
      const userPrompt = `CONTEXT:\n${contextString}\n\nQUESTION:\n${question}`;

      const stream = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
        stream: true,
      });
      
      console.log('\nüí¨ Answer:\n');
      for await (const chunk of stream) {
        process.stdout.write(chunk.choices[0]?.delta?.content || '');
      }
      console.log('\n');

    } catch (error) {
      console.error('‚ùå An error occurred:', error);
    } finally {
      await client.end();
    }
  });

// --- TASK MANAGEMENT COMMANDS ---
const task = program.command('task').description('Manage project tasks');

task
  .command('add')
  .description('Add a new task to a project')
  .argument('<title>', 'The title of the task')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .action(async (title: string, options: { project: string }) => {
    const client = new Client({ connectionString });
    try {
        await client.connect();
        const projectId = await getProjectId(options.project, client);
        
        const titleEmbedding = await getEmbedding(title);

        const result = await client.query(
            'INSERT INTO tasks (project_id, title, embedding) VALUES ($1, $2, $3) RETURNING task_number',
            [projectId, title, pgvector.toSql(titleEmbedding)]
        );
        const taskNumber = result.rows[0].task_number;
        console.log(`‚úÖ Created task #${taskNumber}: "${title}"`);
    } catch (error) {
        console.error('‚ùå Could not add task:', error);
        console.error('NOTE: This might be due to a missing `embedding` column on the `tasks` table. Please run: ALTER TABLE tasks ADD COLUMN embedding vector(1536);');
    } finally {
        await client.end();
    }
  });

task
  .command('list')
  .description('List tasks for a project')
  .requiredOption('-p, --project <source>', 'The project source (Git URL or local path)')
  .option('--status <status>', 'Filter by status (e.g., open, done)', 'open')
  .action(async (options: { project: string, status: string }) => {
    const client = new Client({ connectionString });
    try {
        await client.connect();
        const projectId = await getProjectId(options.project, client);
        const { rows } = await client.query(
            'SELECT task_number, title, status FROM tasks WHERE project_id = $1 AND status = $2 ORDER BY task_number ASC',
            [projectId, options.status]
        );

        if (rows.length === 0) {
            console.log(`No '${options.status}' tasks found for project: ${options.project}`);
            return;
        }

        console.log(`\nTasks for project: ${options.project} [Status: ${options.status}]`);
        console.log('--------------------------------------------------');
        rows.forEach(t => {
            const status = `[${t.status.toUpperCase()}]`.padEnd(7);
            console.log(`#${t.task_number.toString().padEnd(4)} ${status} ${t.title}`);
        });
        console.log('--------------------------------------------------');
    } catch (error) {
        console.error('‚ùå Could not list tasks:', error);
    } finally {
        await client.end();
    }
  });

program.parse(process.argv);

--- FILE: server.ts ---

// src/server.ts
import app from './app';

const port = process.env.PORT || 3000;

app.listen(port, () => {
    console.log(`üß† AI Project Brain is listening at http://localhost:${port}`);
});

--- FILE: middleware/errorHandler.ts ---

// src/middleware/errorHandler.ts
import { Request, Response, NextFunction } from 'express';

export function errorHandler(err: Error, req: Request, res: Response, next: NextFunction) {
    console.error(err.stack);
    res.status(500).json({ error: err.message || 'An internal server error occurred.' });
}

--- FILE: types/tree-sitter-typescript.d.ts ---

declare module 'tree-sitter-typescript/typescript';

--- FILE: core/chunker.ts ---

// --- FILE: core/chunker.ts ---
import Parser, { Query } from 'tree-sitter'; // MODIFIED: Import the Query class
import TypeScript from 'tree-sitter-typescript/typescript';

export interface CodeChunk {
  content: string;
  metadata: {
    type: 'function' | 'class' | 'method' | 'arrow_function' | 'block';
    name: string;
    start_line: number;
    end_line: number;
  };
}

const parser = new Parser();
parser.setLanguage(TypeScript);

// CRITICAL: This query identifies the distinct, semantic blocks of code.
const TS_QUERY = `
[
  (function_declaration) @chunk
  (class_declaration) @chunk
  (method_definition) @chunk
  (lexical_declaration 
    (variable_declarator 
      value: (arrow_function)
    )
  ) @chunk
]
`;

export function chunkCodeWithAST(content: string): CodeChunk[] {
  const tree = parser.parse(content);
  // MODIFIED: Use the Query constructor for robustness
  const query = new Query(TypeScript, TS_QUERY);
  const matches = query.captures(tree.rootNode);

  const chunks: CodeChunk[] = [];

  for (const match of matches) {
    const node = match.node;
    let name = 'anonymous';
    let type: CodeChunk['metadata']['type'] = 'block';

    // Heuristics to find the name of the chunk
    if (node.type === 'function_declaration' || node.type === 'class_declaration' || node.type === 'method_definition') {
        name = node.childForFieldName('name')?.text || 'anonymous';
        if (node.type === 'method_definition') type = 'method';
        else if (node.type === 'class_declaration') type = 'class';
        else type = 'function';
    } else if (node.type === 'lexical_declaration') {
        // For arrow functions like `const myFunc = () => ...`
        name = node.firstNamedChild?.firstNamedChild?.text || 'anonymous_arrow_function';
        type = 'arrow_function';
    }
    
    chunks.push({
      content: node.text,
      metadata: {
        type: type,
        name: name,
        start_line: node.startPosition.row + 1,
        end_line: node.endPosition.row + 1,
      },
    });
  }

  // Fallback: If no specific chunks were found (e.g., a simple config file),
  // treat the entire file as a single chunk.
  if (chunks.length === 0 && content.trim().length > 0) {
    chunks.push({
      content: content,
      metadata: {
        type: 'block',
        name: 'file_content',
        start_line: 1,
        end_line: content.split('\n').length,
      },
    });
  }

  return chunks;
}

--- FILE: core/documentExtractor.ts ---

// --- FILE: core/documentExtractor.ts ---
import fs from 'fs';
import path from 'path';
import mammoth from 'mammoth';
import pdf from 'pdf-parse';
import TurndownService from 'turndown';

export class UnsupportedFileTypeError extends Error {
    constructor(message: string) {
        super(message);
        this.name = 'UnsupportedFileTypeError';
    }
}

// MODIFIED: Create a Turndown instance and add a custom rule.
const turndownService = new TurndownService();

// This new rule identifies paragraphs that ONLY contain bold text and treats them as H2 headings.
// This is a robust way to handle documents where users bold text for headers instead of using proper styles.
turndownService.addRule('strongIsHeading', {
    filter: (node, options) => {
        // Check if the node is a paragraph
        if (node.nodeName !== 'P') {
            return false;
        }
        // Check if it has exactly one child
        if (node.childNodes.length !== 1) {
            return false;
        }
        const child = node.firstChild!;
        // Check if that single child is a <strong> tag
        if (child.nodeName !== 'STRONG') {
            return false;
        }
        // Optional: Check if the heading text is reasonably short
        return (child.textContent || '').length < 200;
    },
    replacement: (content) => {
        // Replace it with a markdown H2 heading
        return `## ${content}\n\n`;
    }
});


/**
 * Extracts clean text content from a file based on its extension.
 * Supports .txt, .md, .docx, and .pdf files.
 * @param filePath The path to the file on disk (the temporary file).
 * @param originalFilename The original name of the file, used to determine the extension.
 * @returns A promise that resolves with the extracted text content.
 * @throws {UnsupportedFileTypeError} if the file extension is not supported.
 */
export async function extractTextFromFile(filePath: string, originalFilename: string): Promise<string> {
    const extension = path.extname(originalFilename).toLowerCase();
    const buffer = fs.readFileSync(filePath);

    switch (extension) {
        case '.docx':
            // The styleMap is a good fallback, but our new Turndown rule is more robust for this case.
            const options = {
                styleMap: [
                    "p[style-name='Heading 1'] => h1:fresh",
                    "p[style-name='Heading 2'] => h2:fresh",
                    "p[style-name='Heading 3'] => h3:fresh",
                    "p[style-name='heading 1'] => h1:fresh",
                    "p[style-name='heading 2'] => h2:fresh",
                    "p[style-name='heading 3'] => h3:fresh",
                ]
            };
            const htmlResult = await mammoth.convertToHtml({ buffer }, options);
            // Use our customized turndown service to convert HTML to Markdown
            return turndownService.turndown(htmlResult.value);

        case '.pdf':
            const data = await pdf(buffer);
            return data.text.replace(/(\s*\n){3,}/g, '\n\n').trim();

        case '.txt':
        case '.md':
        case '.json':
        case '.ts':
        case '.js':
        case '.py':
        case '.html':
        case '.css':
            return buffer.toString('utf-8');
            
        default:
            throw new UnsupportedFileTypeError(`File type "${extension}" is not supported for document ingestion.`);
    }
}

--- FILE: core/textChunker.ts ---

// --- FILE: core/textChunker.ts ---
import { remark } from 'remark';
import { toString } from 'mdast-util-to-string';
import { Root } from 'mdast';

/**
 * A fallback chunker for plain text with no discernible structure.
 * It splits the content by paragraphs (double newlines).
 */
function chunkByParagraphs(content: string): string[] {
    return content
        .split(/\n\s*\n/)
        .map(chunk => chunk.trim())
        .filter(chunk => chunk.length > 20); // Filter out very short lines
}

/**
 * A sophisticated chunker that understands Markdown structure.
 * It groups content under headings to create semantically meaningful chunks.
 * For example, a "## Section Title" and all its following paragraphs and lists
 * will be combined into a single chunk.
 */
function chunkByHeadings(markdownContent: string): string[] {
    const tree = remark().parse(markdownContent) as Root;
    const chunks: string[] = [];
    let currentChunk: any[] = [];

    tree.children.forEach(node => {
        // A heading of level 1, 2, or 3 now starts a new chunk for better granularity.
        if (node.type === 'heading' && node.depth <= 3) {
            if (currentChunk.length > 0) {
                chunks.push(toString({ type: 'root', children: currentChunk }));
            }
            currentChunk = [node];
        } else {
            currentChunk.push(node);
        }
    });

    if (currentChunk.length > 0) {
        chunks.push(toString({ type: 'root', children: currentChunk }));
    }

    // --- NEW LOGGING: START ---
    console.log(`[textChunker] Initial chunking pass found ${chunks.length} potential chunks based on headings.`);
    // --- NEW LOGGING: END ---

    // If the document had no headings, the whole thing is one chunk.
    // In that case, we fall back to the paragraph chunker for better granularity.
    if (chunks.length === 1 && tree.children.some(node => node.type !== 'heading')) {
        // --- NEW LOGGING: START ---
        console.log('[textChunker] Only one heading-based chunk found. Falling back to paragraph-based chunking for better granularity.');
        // --- NEW LOGGING: END ---
        return chunkByParagraphs(chunks[0]);
    }
    
    return chunks.filter(chunk => chunk.trim().length > 0);
}

/**
 * The main text chunking function. It attempts to chunk by Markdown headings first,
 * and falls back to a simpler paragraph-based chunker if needed.
 * @param content The text or markdown content to be chunked.
 * @returns An array of string chunks.
 */
export function chunkText(content: string): string[] {
    // Trim initial/final whitespace which can affect parsing
    const trimmedContent = content.trim();
    if (!trimmedContent) {
        return [];
    }
    return chunkByHeadings(trimmedContent);
}

--- FILE: core/prompts/fileSummary.prompt.ts ---

// --- FILE: core/prompts/fileSummary.prompt.ts ---

/**
 * Generates the prompt for summarizing a single code file.
 * @param filePath The relative path of the file.
 * @param content The full content of the file.
 * @returns A formatted string ready to be sent to the OpenAI API.
 */
export function generateFileSummaryPrompt(filePath: string, content: string): string {
    return `Summarize the purpose of the following code file in one sentence. Be concise and focus on the file's primary role or responsibility.

File Path: ${filePath}

--- CODE START ---
${content}
--- CODE END ---

One-sentence summary:`;
}

--- FILE: core/prompts/taskGeneration.prompt.ts ---

// --- FILE: core/prompts/taskGeneration.prompt.ts ---

/**
 * Generates the full prompt for the AI to analyze a git commit and create a structured task.
 * @param commitMessage The message from the git commit.
 * @param gitDiff The full text diff of the changes in the commit.
 * @returns A formatted string ready to be sent to the OpenAI API.
 */
export function generateTaskFromCommitPrompt(commitMessage: string, gitDiff: string): string {
    return `You are an expert software engineering project manager analyzing a git commit. Your goal is to generate a concise task title, a category, and a detailed description of the work.

**Instructions & Rules:**

1.  **Analyze**: Carefully review the commit message and the git diff to understand the full context of the work performed.
2.  **Output Format**: You MUST respond ONLY with a single JSON object. Do not include any explanatory text, markdown syntax, or anything outside of the JSON structure.
3.  **Trivial Commits**: If the diff is truly trivial (e.g., only a typo fix in a comment, a whitespace change), respond with the exact string "NULL" instead of a JSON object.
4.  **JSON Structure**: The JSON object must have three keys: "title", "category", and "description".

    *   **"title" field rules:**
        *   MUST start with an imperative verb (e.g., "Add", "Refactor", "Fix", "Update", "Implement").
        *   MUST be a single, concise line summarizing the work done.

    *   **"category" field rules:**
        *   MUST be one of the following exact strings: 'feature', 'fix', 'refactor', 'chore', 'docs', 'test'.

    *   **"description" field rules:**
        *   MUST be a brief, one or two-sentence summary of the changes.
        *   This summary SHOULD be followed by a short markdown bulleted list of the most important changes.
        *   Example: "This commit introduces a new queueing system to handle concurrent ingestion tasks. Key changes include:\n- Added 'p-queue' library.\n- Created a singleton queue service.\n- Wrapped the ingestion logic in the project controller."

**JSON Output Structure Example:**
\`\`\`json
{
  "title": "Implement a queue for ingestion tasks",
  "category": "feature",
  "description": "Introduces a job queue to manage concurrent project sync requests, preventing system crashes under load. Key changes include:\\n- Added 'p-queue' dependency.\\n- Created a global singleton queue service with a concurrency of 1.\\n- Modified the project controller to add ingestion jobs to the queue."
}
\`\`\`

---
**INPUT DATA**
---

**COMMIT MESSAGE:**
\`\`\`
${commitMessage}
\`\`\`

**GIT DIFF:**
\`\`\`diff
${gitDiff}
\`\`\`
`;
}

--- FILE: scripts/ingest.ts ---

// --- FILE: scripts/ingest.ts ---

import 'dotenv/config';
import { glob } from 'glob';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { Client } from 'pg';
import OpenAI from 'openai';
import gitignore from 'gitignore-parser';
import pgvector from 'pgvector/pg';
import { chunkCodeWithAST } from '../core/chunker';
import simpleGit, { SimpleGit, LogResult, DefaultLogFields } from 'simple-git';
// REFACTORED: Import both dedicated prompt generators
import { generateFileSummaryPrompt } from '../core/prompts/fileSummary.prompt';
import { generateTaskFromCommitPrompt } from '../core/prompts/taskGeneration.prompt';

// --- CONFIGURATION (Global) ---
const connectionString = process.env.DATABASE_URL!;
const openaiApiKey = process.env.OPENAI_API_KEY!;

if (!connectionString || !openaiApiKey) {
  throw new Error('FATAL: Missing environment variables DATABASE_URL or OPENAI_API_KEY');
}

const openai = new OpenAI({ apiKey: openaiApiKey });
const IGNORED_EXTENSIONS = new Set(['.lock', '.svg', '.png', '.jpg', '.jpeg', '.gif', '.ico']);
const IGNORED_FILENAMES = new Set(['package-lock.json', 'yarn.lock', 'pnpm-lock.yaml']);

export type IngestionLogger = (message: string) => void;

// --- CORE HELPER FUNCTIONS ---
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

// REFACTORED: This function now uses the imported prompt
async function summarizeFile(filePath: string, content: string, logger: IngestionLogger): Promise<string> {
  const prompt = generateFileSummaryPrompt(filePath, content);
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 100,
      temperature: 0.1,
    });
    return response.choices[0].message.content?.trim() || "Could not generate a summary.";
  } catch (error) {
    logger(`  - Failed to summarize ${filePath}: ${error instanceof Error ? error.message : String(error)}`);
    return "Summary generation failed.";
  }
}

// --- MAIN INGESTION LOGIC ---
export async function runIngestion(projectId: number, projectPath: string, logger: IngestionLogger) {
  const client = new Client({ connectionString });
  await client.connect();
  await pgvector.registerType(client);
  logger('Database connection established.');

  const git: SimpleGit = simpleGit(projectPath);

  try {
    await syncFiles(client, projectId, projectPath, logger);
    await syncGitHistory(client, projectId, git, logger);
  } finally {
    logger('Ingestion process finished. Closing database connection.');
    await client.end();
  }
}

// --- STAGE 1: Sync Filesystem State ---
// This function remains the same, but its call to summarizeFile is now cleaner.
async function syncFiles(client: Client, projectId: number, projectPath: string, logger: IngestionLogger) {
  logger(`[1/4] Starting file sync for project ID: ${projectId}`);
  
  logger(`[2/4] Pruning deleted files from the database...`);
  const { rows: dbFiles } = await client.query('SELECT path FROM indexed_files WHERE project_id = $1', [projectId]);
  const dbPaths = new Set(dbFiles.map(f => f.path));
  
  const allDiskFiles = await glob('**/*', { cwd: projectPath, nodir: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] });
  const diskPaths = new Set(allDiskFiles);

  const pathsToDelete = [...dbPaths].filter(p => !diskPaths.has(p));

  if (pathsToDelete.length > 0) {
    logger(`      Found ${pathsToDelete.length} files to delete.`);
    await client.query('DELETE FROM indexed_files WHERE project_id = $1 AND path = ANY($2::text[])', [projectId, pathsToDelete]);
    logger(`      -> Pruning complete.`);
  } else {
    logger(`      -> No files to prune.`);
  }

  const gitignorePath = path.join(projectPath, '.gitignore');
  const ignore = fs.existsSync(gitignorePath)
    ? gitignore.compile(fs.readFileSync(gitignorePath, 'utf8'))
    : { accepts: (_p: string) => true };

  const filesToIndex = allDiskFiles.filter(file => {
    const ext = path.extname(file);
    const filename = path.basename(file);
    return ignore.accepts(file) && !IGNORED_EXTENSIONS.has(ext) && !IGNORED_FILENAMES.has(filename);
  });

  logger(`[3/4] Found ${filesToIndex.length} files to process for additions/modifications.`);
  let processedCount = 0;

  for (const relativePath of filesToIndex) {
    const fullPath = path.join(projectPath, relativePath);
    const content = fs.readFileSync(fullPath, 'utf-8');
    if (!content.trim()) continue;

    const hash = crypto.createHash('sha256').update(content).digest('hex');
    const { rows } = await client.query('SELECT content_hash FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, relativePath]);

    if (rows.length > 0 && rows[0].content_hash === hash) {
      continue;
    }

    processedCount++;
    logger(`      Processing changed file: ${relativePath}`);

    await client.query('BEGIN');
    try {
      await client.query('DELETE FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, relativePath]);
      
      const summary = await summarizeFile(relativePath, content, logger);
      const summaryEmbedding = await getEmbedding(summary);
      
      const fileInsertResult = await client.query(
        'INSERT INTO indexed_files (project_id, path, content_hash, summary, summary_embedding, last_indexed_at) VALUES ($1, $2, $3, $4, $5, NOW()) RETURNING id',
        [projectId, relativePath, hash, summary, pgvector.toSql(summaryEmbedding)]
      );
      const fileId = fileInsertResult.rows[0].id;
      
      const chunks = chunkCodeWithAST(content);
      for (const chunk of chunks) {
        const chunkEmbedding = await getEmbedding(chunk.content);
        await client.query(
          `INSERT INTO code_chunks (file_id, chunk_name, chunk_type, content, start_line, end_line, embedding) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
          [fileId, chunk.metadata.name, chunk.metadata.type, chunk.content, chunk.metadata.start_line, chunk.metadata.end_line, pgvector.toSql(chunkEmbedding)]
        );
      }
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      logger(`      Failed to process ${relativePath}: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
  logger(`[4/4] File sync complete. Processed ${processedCount} new or changed files.`);
}

// --- STAGE 2: Sync Git Commit History ---

// REFACTORED: This function now uses the imported prompt
async function generateTaskFromCommit(client: Client, projectId: number, commit: DefaultLogFields, git: SimpleGit, logger: IngestionLogger) {
    const diff = await git.show(['--patch', '--first-parent', commit.hash]);
    
    if (!diff || diff.trim().length < 50) { 
        logger(`      -> Commit ${commit.hash.substring(0,7)} is trivial, skipping task generation.`);
        return;
    }

    const prompt = generateTaskFromCommitPrompt(commit.message, diff);

    try {
        const response = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: [{ role: 'user', content: prompt }],
            response_format: { type: 'json_object' },
            max_tokens: 400, // Increased token limit for more detailed descriptions
            temperature: 0.1,
        });
        
        const responseText = response.choices[0].message.content?.trim();

        if (!responseText || responseText.toUpperCase() === 'NULL') {
            logger(`      -> AI determined commit is trivial, skipping task generation.`);
            return;
        }

        // MODIFIED: Destructure the new 'description' field
        const { title, category, description } = JSON.parse(responseText);

        if (!title || !category || !description) {
            throw new Error('AI response was missing title, category, or description.');
        }

        logger(`      -> AI generated task: [${category}] "${title}"`);

        // MODIFIED: The content to embed now includes the more detailed description
        const contentToEmbed = `[${category}] ${title}\n\n${description}\n\nCompleted in commit: ${commit.hash}`;
        const taskEmbedding = await getEmbedding(contentToEmbed);

        await client.query(
            `INSERT INTO tasks (project_id, title, description, status, category, embedding, created_at, updated_at) 
             VALUES ($1, $2, $3, 'done', $4, $5, $6, $6)`,
            [
                projectId, 
                title, 
                // MODIFIED: Use the AI-generated description
                description,
                category,
                pgvector.toSql(taskEmbedding),
                commit.date
            ]
        );
        logger(`      ‚úÖ Created and closed retrospective task for commit ${commit.hash.substring(0,7)}.`);

    } catch (error) {
        logger(`      ‚ùå Failed to generate task for commit ${commit.hash.substring(0,7)}: ${error instanceof Error ? error.message : 'Unknown AI or parsing error'}`);
    }
}

// This function now contains the core orchestration logic for git history.
async function syncGitHistory(client: Client, projectId: number, git: SimpleGit, logger: IngestionLogger) {
    logger('\n[1/3] Starting Git history sync...');
    
    const { rows: existingCommits } = await client.query('SELECT commit_hash FROM commits WHERE project_id = $1', [projectId]);
    const existingHashes = new Set(existingCommits.map(c => c.commit_hash));
    logger(`[2/3] Found ${existingHashes.size} existing commits in the database.`);

    const log: LogResult<DefaultLogFields> = await git.log();
    const allCommits = [...log.all].reverse();

    const newCommits = allCommits.filter(c => !existingHashes.has(c.hash));
    if (newCommits.length === 0) {
        logger('[3/3] Git history is already up-to-date.');
        return;
    }
    logger(`      Found ${newCommits.length} new commits to process.`);

    for (const commit of newCommits) {
        logger(`      Processing commit ${commit.hash.substring(0, 7)}: ${commit.message}`);
        
        await client.query('BEGIN');
        try {
            const messageEmbedding = await getEmbedding(commit.message);
            const commitInsertResult = await client.query(
                `INSERT INTO commits (project_id, commit_hash, author_name, author_email, commit_date, message, embedding) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id`,
                [projectId, commit.hash, commit.author_name, commit.author_email, commit.date, commit.message, pgvector.toSql(messageEmbedding)]
            );
            const commitId = commitInsertResult.rows[0].id;
            
            const diffSummary = await git.show(['--name-status', '--pretty=format:', commit.hash]);
            const changedFiles = diffSummary.split('\n').filter(line => line.trim());

            for (const line of changedFiles) {
                const parts = line.split('\t');
                if (parts.length < 2) continue;
                const change_type = parts[0].trim();
                const file_path = parts[1].trim();
                
                const { rows } = await client.query('SELECT id FROM indexed_files WHERE project_id = $1 AND path = $2', [projectId, file_path]);

                if (rows.length > 0) {
                    const fileId = rows[0].id;
                    await client.query(
                        `INSERT INTO commit_files (commit_id, file_id, change_type) VALUES ($1, $2, $3)`,
                        [commitId, fileId, change_type]
                    );
                }
            }

            const taskRegex = /(?:closes|fixes|resolves)\s+#(\d+)/gi;
            const match = taskRegex.exec(commit.message);

            if (match) {
                const taskNumber = parseInt(match[1], 10);
                logger(`      -> Found reference to close task #${taskNumber}.`);
                const updateResult = await client.query(
                    `UPDATE tasks SET status = 'done', updated_at = NOW() WHERE project_id = $1 AND task_number = $2 AND status != 'done'`,
                    [projectId, taskNumber]
                );
                if (updateResult.rowCount && updateResult.rowCount > 0) {
                    logger(`      ‚úÖ Automatically closed task #${taskNumber}.`);
                }
            } else {
                await generateTaskFromCommit(client, projectId, commit, git, logger);
            }

            await client.query('COMMIT');
        } catch (error) {
            await client.query('ROLLBACK');
            logger(`      Failed to process commit ${commit.hash}: ${error instanceof Error ? error.message : String(error)}`);
        }
    }
    logger('[3/3] Git history sync complete.');
}

--- FILE: api/tasks/task.controller.ts ---

// --- FILE: api/tasks/task.controller.ts ---

// src/api/tasks/task.controller.ts
import { Request, Response, NextFunction } from 'express';
import * as taskService from './task.service';

export async function listTasks(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const status = req.query.status as string | undefined; // Allow undefined
        const tasks = await taskService.getTasks(projectId, status);
        res.json(tasks);
    } catch (error) {
        next(error);
    }
}

// MODIFIED: Handle 'description' field from the body
export async function createTask(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const { title, description } = req.body; // <-- Get description here
        if (!title) {
            return res.status(400).json({ error: 'A "title" is required.' });
        }
        const newTask = await taskService.createTask(projectId, title, description); // <-- Pass it here
        res.status(201).json(newTask);
    } catch (error) {
        next(error);
    }
}

// MODIFIED: Handle multiple fields for update
export async function updateTask(req: Request, res: Response, next: NextFunction) {
    try {
        const { projectId, taskNumber } = req.params;
        const { title, description, status } = req.body; // <-- Get all potential updates
        
        const updatedTask = await taskService.updateTask(
            parseInt(projectId, 10),
            parseInt(taskNumber, 10),
            { title, description, status } // <-- Pass as an object
        );
        res.json(updatedTask);
    } catch (error) {
        next(error);
    }
}

export async function deleteTask(req: Request, res: Response, next: NextFunction) {
    try {
        const { projectId, taskNumber } = req.params;
        await taskService.deleteTask(
            parseInt(projectId, 10),
            parseInt(taskNumber, 10)
        );
        // 204 No Content is the standard successful response for a DELETE request
        res.status(204).send();
    } catch (error) {
        next(error);
    }
}

--- FILE: api/tasks/task.routes.ts ---

// src/api/tasks/task.routes.ts
import { Router } from 'express';
import * as taskController from './task.controller';

const router = Router({ mergeParams: true }); // mergeParams is crucial for nested routes

router.get('/', taskController.listTasks);
router.post('/', taskController.createTask); 
router.put('/:taskNumber', taskController.updateTask); 
router.delete('/:taskNumber', taskController.deleteTask);

export default router;

--- FILE: api/tasks/task.service.ts ---

// src/api/tasks/task.service.ts
import pool from '../../services/db';
import { getEmbedding } from '../../services/openai';
import pgvector from 'pgvector/pg';

export async function getTasks(projectId: number, status?: string) {
    const client = await pool.connect();
    try {
        let query = 'SELECT * FROM tasks WHERE project_id = $1';
        const params: any[] = [projectId];
        if (status) {
            query += ' AND status = $2';
            params.push(status);
        }
        query += ' ORDER BY task_number ASC';
        const { rows } = await client.query(query, params);
        return rows;
    } finally {
        client.release();
    }
}

export async function createTask(projectId: number, title: string, description?: string) {
    const client = await pool.connect();
    try {
        const contentToEmbed = `${title}${description ? `\n\n${description}` : ''}`;
        const titleEmbedding = await getEmbedding(contentToEmbed);
        const { rows } = await client.query(
            'INSERT INTO tasks (project_id, title, description, embedding) VALUES ($1, $2, $3, $4) RETURNING *',
            [projectId, title, description || null, pgvector.toSql(titleEmbedding)]
        );
        return rows[0];
    } catch (error) {
        console.error('Task creation failed. Ensure the `tasks` table has `description` and `embedding` columns.');
        throw error;
    }
    finally {
        client.release();
    }
}

interface TaskUpdates {
    title?: string;
    description?: string;
    status?: 'open' | 'in_progress' | 'done';
}

export async function updateTask(projectId: number, taskNumber: number, updates: TaskUpdates) {
    const client = await pool.connect();
    try {
        const { rows: existingTasks } = await client.query(
            'SELECT title, description FROM tasks WHERE project_id = $1 AND task_number = $2',
            [projectId, taskNumber]
        );

        if (existingTasks.length === 0) {
            throw new Error('Task not found');
        }

        const currentTask = existingTasks[0];
        const newTitle = updates.title || currentTask.title;
        const newDescription = updates.description || currentTask.description;

        const fieldsToUpdate = [];
        const values = [];
        let paramIndex = 1;

        if (updates.status) {
            if (!['open', 'in_progress', 'done'].includes(updates.status)) {
                throw new Error('Invalid task status');
            }
            fieldsToUpdate.push(`status = $${paramIndex++}`);
            values.push(updates.status);
        }
        if (updates.title) {
            fieldsToUpdate.push(`title = $${paramIndex++}`);
            values.push(updates.title);
        }
        if (updates.description) {
            fieldsToUpdate.push(`description = $${paramIndex++}`);
            values.push(updates.description);
        }

        if (updates.title || updates.description) {
            const contentToEmbed = `${newTitle}${newDescription ? `\n\n${newDescription}` : ''}`;
            const newEmbedding = await getEmbedding(contentToEmbed);
            fieldsToUpdate.push(`embedding = $${paramIndex++}`);
            values.push(pgvector.toSql(newEmbedding));
        }

        if (fieldsToUpdate.length === 0) {
            return currentTask;
        }

        fieldsToUpdate.push(`updated_at = NOW()`);
        
        values.push(projectId, taskNumber);

        const query = `UPDATE tasks SET ${fieldsToUpdate.join(', ')} WHERE project_id = $${paramIndex++} AND task_number = $${paramIndex++} RETURNING *`;
        
        const { rows } = await client.query(query, values);

        return rows[0];
    } finally {
        client.release();
    }
}

export async function deleteTask(projectId: number, taskNumber: number): Promise<void> {
    const client = await pool.connect();
    try {
        const result = await client.query(
            'DELETE FROM tasks WHERE project_id = $1 AND task_number = $2',
            [projectId, taskNumber]
        );

        if (result.rowCount === 0) {
            throw new Error('Task not found or does not belong to this project.');
        }
    } finally {
        client.release();
    }
}

--- FILE: api/projects/project.controller.ts ---

// src/api/projects/project.controller.ts
import { Request, Response, NextFunction } from 'express';
import * as projectService from './project.service';
import * as qaService from './qa.service';
import ingestionQueue from '../../services/queue.service';
import { UnsupportedFileTypeError } from '../../core/documentExtractor';


export async function listProjects(req: Request, res: Response, next: NextFunction) {
    try {
        const projects = await projectService.getAllProjects();
        res.json(projects);
    } catch (error) {
        next(error);
    }
}

export async function addProject(req: Request, res: Response, next: NextFunction) {
    try {
        const { source } = req.body;
        if (!source) {
            return res.status(400).json({ error: 'A "source" Git URL is required.' });
        }
        
        const { project, created } = await projectService.createProject(source);

        if (!created) {
            return res.status(200).json({ message: 'Project already exists.', project });
        }
        
        // Respond immediately and start ingestion in the background
        res.status(202).json({ message: 'Project created. Ingestion will start in the background.', project });
        projectService.startProjectIngestionInBackground(project.id, project.source);

    } catch (error) {
        next(error);
    }
}

// REMOVED old syncProject, which is replaced by streamIngestionLogs

// NEW: Controller for streaming ingestion logs
export async function streamIngestionLogs(req: Request, res: Response, next: NextFunction) {
    const projectId = parseInt(req.params.projectId, 10);
    
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.flushHeaders();

    const logger = (message: string) => {
        if (!res.writableEnded) {
            res.write(`data: ${JSON.stringify({ log: message })}\n\n`);
        }
    };

    req.on('close', () => {
        console.log(`Client disconnected from ingestion stream for project ${projectId}.`);
        res.end();
    });

    // MODIFIED: Wrap the entire ingestion logic in the queue
    ingestionQueue.add(async () => {
        try {
            const project = await projectService.getProjectById(projectId);
            if (!project) {
                logger(`Error: Project with ID ${projectId} not found.`);
                return; // Return from the job, not the outer function
            }
            
            logger('Your sync request is now being processed...');
            await projectService.startProjectIngestion(projectId, project.source, logger);
            
            logger('Ingestion complete.');
            res.write('event: end\ndata: {"message": "Ingestion complete"}\n\n');

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            logger(`FATAL ERROR: ${errorMessage}`);
            console.error("Error during ingestion stream:", error);
            res.write(`event: error\ndata: {"message": "${errorMessage}"}\n\n`);
        } finally {
            if (!res.writableEnded) {
                res.end();
            }
        }
    }).catch((err:any) => {
        // This catch is for errors adding the job to the queue itself, which is rare.
        console.error("Failed to add ingestion job to queue:", err);
        if (!res.writableEnded) {
            res.status(500).send("Failed to queue the ingestion job.");
        }
    });
}


// REMOVED: The `askQuestion` controller has been deleted from this file.
// Its functionality is now handled by the new conversation controller.


export async function uploadDocument(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        if (!req.file) {
            return res.status(400).json({ error: 'No file uploaded.' });
        }
        
        const document = await projectService.addProjectDocument(
            projectId,
            req.file.originalname,
            req.file.path
        );

        res.status(201).json({ message: 'Document uploaded and indexed successfully.', document });
    } catch (error) {
        // MODIFIED: Catch specific error for unsupported file types
        if (error instanceof UnsupportedFileTypeError) {
            return res.status(400).json({ error: error.message });
        }
        next(error);
    }
}

// NEW: Controller to list all documents for a project
export async function listDocuments(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const documents = await projectService.getProjectDocuments(projectId);
        res.json(documents);
    } catch (error) {
        next(error);
    }
}

// NEW: Controller to delete a document
export async function deleteDocument(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const documentId = parseInt(req.params.documentId, 10);
        await projectService.deleteProjectDocument(projectId, documentId);
        res.status(204).send(); // 204 No Content is standard for successful deletions
    } catch (error) {
        next(error);
    }
}


// NEW: Controller for project stats
export async function getProjectStats(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const stats = await projectService.getProjectStats(projectId);
        res.json(stats);
    } catch (error) {
        next(error);
    }
}

--- FILE: api/projects/project.routes.ts ---

// src/api/projects/project.routes.ts
import { Router } from 'express';
import * as projectController from './project.controller';
import taskRoutes from '../tasks/task.routes';
import conversationRoutes from '../conversations/conversation.routes'; // <-- IMPORT NEW ROUTES
import multer from 'multer';

const router = Router();
const upload = multer({ dest: 'uploads/' });

router.get('/', projectController.listProjects);
router.post('/', projectController.addProject);

// MODIFIED: This is now a GET request to establish an SSE connection for logs
router.get('/:projectId/sync-stream', projectController.streamIngestionLogs);

// NEW: Route to get project statistics
router.get('/:projectId/stats', projectController.getProjectStats);

// REMOVED: The old 'ask' route is gone.
// router.post('/:projectId/ask', projectController.askQuestion);

// --- Document Routes ---
router.post('/:projectId/documents', upload.single('document'), projectController.uploadDocument);
// NEW: Route to list all documents for a project
router.get('/:projectId/documents', projectController.listDocuments);
// NEW: Route to delete a specific document
router.delete('/:projectId/documents/:documentId', projectController.deleteDocument);


// Mount task routes nested under projects
router.use('/:projectId/tasks', taskRoutes);

// NEW: Mount conversation routes nested under projects
router.use('/:projectId/conversations', conversationRoutes);

export default router;

--- FILE: api/projects/project.service.ts ---

// src/api/projects/project.service.ts
import pool from '../../services/db';
import { cloneOrPullRepo } from '../../services/git';
import { runIngestion, IngestionLogger } from '../../scripts/ingest';
import path from 'path';
import { chunkText } from '../../core/textChunker';
import { getEmbedding } from '../../services/openai';
import fs from 'fs/promises';
import pgvector from 'pgvector/pg';
import { extractTextFromFile } from '../../core/documentExtractor';

export async function getAllProjects() {
    const client = await pool.connect();
    try {
        const { rows } = await client.query('SELECT id, name, source FROM projects ORDER BY created_at DESC');
        return rows;
    } finally {
        client.release();
    }
}

export async function getProjectById(projectId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query('SELECT * FROM projects WHERE id = $1', [projectId]);
        if (rows.length === 0) {
            return null;
        }
        return rows[0];
    } finally {
        client.release();
    }
}


export async function createProject(source: string) {
    const client = await pool.connect();
    try {
        const existing = await client.query('SELECT * FROM projects WHERE source = $1', [source]);
        if (existing.rows.length > 0) {
            return { project: existing.rows[0], created: false };
        }

        const projectName = path.basename(source, path.extname(source));
        const { rows } = await client.query(
            'INSERT INTO projects (name, source) VALUES ($1, $2) RETURNING *',
            [projectName, source]
        );
        return { project: rows[0], created: true };
    } finally {
        client.release();
    }
}

export async function startProjectIngestion(projectId: number, source: string, logger: IngestionLogger) {
    try {
        const projectPath = await cloneOrPullRepo(source, logger);
        logger(`[Project ${projectId}] Ingestion running...`);
        await runIngestion(projectId, projectPath, logger);
        logger(`‚úÖ [Project ${projectId}] Ingestion complete.`);
    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);
        logger(`‚ùå [Project ${projectId}] Ingestion failed: ${errorMessage}`);
        console.error(`‚ùå [Project ${projectId}] Ingestion failed:`, error);
    }
}

export function startProjectIngestionInBackground(projectId: number, source: string) {
    startProjectIngestion(projectId, source, console.log);
}

export async function addProjectDocument(projectId: number, originalFilename: string, storedFilePath: string) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');

        const content = await extractTextFromFile(storedFilePath, originalFilename);

        const docResult = await client.query(
            'INSERT INTO project_documents (project_id, file_name, file_path) VALUES ($1, $2, $3) RETURNING id',
            [projectId, originalFilename, storedFilePath]
        );
        const documentId = docResult.rows[0].id;

        const chunks = chunkText(content);

        console.log(`[project.service] Number of chunks to be inserted: ${chunks.length}`);
        let chunkCounter = 0;

        for (const chunk of chunks) {
            chunkCounter++;
            console.log(`[project.service] >> Processing chunk ${chunkCounter}/${chunks.length}`);

            // Let's inspect the chunk to make sure it's valid
            if (!chunk || chunk.trim().length < 5) {
                console.log(`[project.service] >> Chunk ${chunkCounter} is too short or empty. Skipping.`);
                continue;
            }

            try {
                console.log(`[project.service] >>   1. Generating embedding for chunk ${chunkCounter}...`);
                const embedding = await getEmbedding(chunk);
                console.log(`[project.service] >>   2. Embedding generated (length: ${embedding.length}). Inserting into DB...`);

                await client.query(
                    'INSERT INTO document_chunks (document_id, content, embedding) VALUES ($1, $2, $3)',
                    [documentId, chunk, pgvector.toSql(embedding)]
                );

                console.log(`[project.service] >>   3. Successfully inserted chunk ${chunkCounter} into DB.`);

            } catch (loopError) {
                // THIS IS A NEW, CRITICAL CATCH BLOCK
                console.error(`[project.service] >> !! ERROR inside chunk loop on chunk ${chunkCounter}:`, loopError);
                // We re-throw the error to ensure the main transaction is rolled back.
                throw loopError;
            }
        }

        console.log(`[project.service] Finished processing all chunks. Committing transaction...`);

        await client.query('COMMIT');
        console.log('[project.service] Transaction committed.'); // Let's confirm this happens
        return { id: documentId, file_name: originalFilename, file_path: storedFilePath };
    } catch (error) {
        await client.query('ROLLBACK');
        console.error(`Failed to process document ${originalFilename}:`, error);
        throw error;
    } finally {
        client.release();
    }
}

export async function getProjectDocuments(projectId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query(
            'SELECT id, file_name, created_at FROM project_documents WHERE project_id = $1 ORDER BY created_at DESC',
            [projectId]
        );
        return rows;
    } finally {
        client.release();
    }
}

export async function deleteProjectDocument(projectId: number, documentId: number) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');

        const docResult = await client.query(
            'SELECT file_path FROM project_documents WHERE id = $1 AND project_id = $2',
            [documentId, projectId]
        );

        if (docResult.rows.length === 0) {
            throw new Error('Document not found or does not belong to this project.');
        }
        const filePath = docResult.rows[0].file_path;

        await client.query('DELETE FROM document_chunks WHERE document_id = $1', [documentId]);
        await client.query('DELETE FROM project_documents WHERE id = $1', [documentId]);

        await client.query('COMMIT');

        if (filePath) {
            try {
                await fs.unlink(filePath);
            } catch (fileError) {
                console.error(`Failed to delete document file ${filePath}:`, fileError);
            }
        }
    } catch (error) {
        await client.query('ROLLBACK');
        console.error(`Failed to delete document ${documentId}:`, error);
        throw error;
    } finally {
        client.release();
    }
}

export async function getProjectStats(projectId: number) {
    const client = await pool.connect();
    try {
        const [
            fileStatsRes,
            taskStatsRes,
            docStatsRes,
            commitHistoryRes,
            contributorRes
        ] = await Promise.all([
            client.query(
                `SELECT
                    (SELECT COUNT(*) FROM indexed_files WHERE project_id = $1) as file_count,
                    (SELECT COUNT(*) FROM code_chunks WHERE file_id IN (SELECT id FROM indexed_files WHERE project_id = $1)) as chunk_count`,
                [projectId]
            ),
            client.query(
                `SELECT status, COUNT(*) as count FROM tasks WHERE project_id = $1 GROUP BY status`,
                [projectId]
            ),
            client.query(
                `SELECT COUNT(*) as document_count FROM project_documents WHERE project_id = $1`,
                [projectId]
            ),
            client.query(
                `SELECT commit_hash, author_name, commit_date, message FROM commits WHERE project_id = $1 ORDER BY commit_date DESC LIMIT 50`,
                [projectId]
            ),
            client.query(
                `SELECT COUNT(DISTINCT author_name) as contributor_count FROM commits WHERE project_id = $1`,
                [projectId]
            )
        ]);

        const taskStats = taskStatsRes.rows.reduce((acc, row) => {
            acc[row.status] = parseInt(row.count, 10);
            return acc;
        }, { open: 0, in_progress: 0, done: 0 });

        return {
            files: {
                count: parseInt(fileStatsRes.rows[0].file_count, 10),
                chunks: parseInt(fileStatsRes.rows[0].chunk_count, 10),
            },
            tasks: taskStats,
            documents: {
                count: parseInt(docStatsRes.rows[0].document_count, 10),
            },
            git: {
                commitCount: commitHistoryRes.rows.length,
                contributorCount: parseInt(contributorRes.rows[0].contributor_count, 10),
                history: commitHistoryRes.rows,
            }
        };
    } finally {
        client.release();
    }
}

--- FILE: api/projects/qa.service.ts ---

// src/api/projects/qa.service.ts
import pool from '../../services/db';
import * as openAI from '../../services/openai';
import pgvector from 'pgvector/pg';
import { PoolClient } from 'pg';
import OpenAI from 'openai';

// NEW: Define a type for the sources we collect.
export interface Source {
    type: 'task' | 'commit' | 'document' | 'code' | 'knowledge'; // <-- Added 'knowledge'
    id: string | number;
    title: string;
}

// NEW: Define a type for message history, matching the one in openai.ts
export interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}


// MODIFIED: The function signature is completely new.
export async function getAnswerStream(
    projectId: number, 
    latestQuestion: string, 
    history: ChatMessage[]
): Promise<{ stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>, sources: Source[] }> {
    let client: PoolClient | null = null;
    try {
        client = await pool.connect();
        const questionEmbedding = await openAI.getEmbedding(latestQuestion);
        let contextString = '';
        const sources: Source[] = [];

        // --- NEW: Search for relevant knowledge notes (past decisions) ---
        try {
            const { rows: relevantNotes } = await client.query(
                `SELECT id, note_summary FROM knowledge_notes WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 2`,
                [projectId, pgvector.toSql(questionEmbedding)]
            );
            if (relevantNotes.length > 0) {
                contextString += "Relevant Past Decisions/Summaries:\n" + relevantNotes.map(n => `- ${n.note_summary}`).join('\n') + '\n\n';
                relevantNotes.forEach(n => sources.push({
                    type: 'knowledge',
                    id: n.id,
                    title: n.note_summary
                }));
            }
        } catch (error: any) {
             if (error.code === '42P01') {
                 console.warn('Warning: knowledge_notes table not found. Skipping knowledge search. Run migrations to enable this feature.');
            } else {
                throw error;
            }
        }

        // --- Existing context retrieval (unchanged logic) ---
        const { rows: relevantTasks } = await client.query(
            `SELECT task_number, title, status FROM tasks WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        if (relevantTasks.length > 0) {
            contextString += "Relevant Tasks:\n" + relevantTasks.map(t => `- Task #${t.task_number} [${t.status.toUpperCase()}]: ${t.title}`).join('\n') + '\n\n';
            relevantTasks.forEach(t => sources.push({
                type: 'task',
                id: t.task_number,
                title: `#${t.task_number}: ${t.title}`
            }));
        }

        const { rows: relevantCommits } = await client.query(
            `SELECT commit_hash, message, author_name FROM commits WHERE project_id = $1 ORDER BY embedding <=> $2 LIMIT 3`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        if (relevantCommits.length > 0) {
            contextString += "Relevant Commits:\n" + relevantCommits.map(c => `- Commit ${c.commit_hash.substring(0, 7)} by ${c.author_name}: ${c.message.split('\n')[0]}`).join('\n') + '\n\n';
            relevantCommits.forEach(c => sources.push({
                type: 'commit',
                id: c.commit_hash.substring(0, 7),
                title: `${c.commit_hash.substring(0, 7)}: ${c.message.split('\n')[0]}`
            }));
        }
        
        try {
            const { rows: relevantDocChunks } = await client.query(
                `SELECT
                   pd.file_name,
                   dc.content
                 FROM document_chunks dc
                 JOIN project_documents pd ON dc.document_id = pd.id
                 WHERE pd.project_id = $1
                 ORDER BY dc.embedding <=> $2
                 LIMIT 3`,
                [projectId, pgvector.toSql(questionEmbedding)]
            );
            if (relevantDocChunks.length > 0) {
                contextString += "Relevant Project Documents:\n" + relevantDocChunks.map(d => `--- FROM DOCUMENT: ${d.file_name} ---\n\n${d.content}`).join('\n\n') + '\n\n';
                const uniqueDocs = [...new Map(relevantDocChunks.map(d => [d.file_name, d])).values()];
                uniqueDocs.forEach(d => sources.push({
                    type: 'document',
                    id: d.file_name,
                    title: d.file_name
                }));
            }
        } catch (error: any) {
            if (error.code === '42P01') {
                 console.warn('Warning: project_documents or document_chunks table not found. Skipping document search. Run migrations to enable this feature.');
            } else {
                throw error;
            }
        }

        const { rows: relevantFiles } = await client.query(
            `SELECT id, path FROM indexed_files WHERE project_id = $1 ORDER BY summary_embedding <=> $2 LIMIT 5`,
            [projectId, pgvector.toSql(questionEmbedding)]
        );
        
        if (relevantFiles.length > 0) {
            const relevantFileIds = relevantFiles.map(f => f.id);
            const { rows: contextChunks } = await client.query(
                `SELECT file_id, content, chunk_name FROM code_chunks WHERE file_id = ANY($1::int[]) ORDER BY embedding <=> $2 LIMIT 10`,
                [relevantFileIds, pgvector.toSql(questionEmbedding)]
            );
            
            if (contextChunks.length > 0) {
                 const chunkContext = contextChunks.map(c => {
                    const filePath = relevantFiles.find(f => f.id === c.file_id)?.path;
                    return `--- FILE: ${filePath} (Chunk: ${c.chunk_name}) ---\n\n${c.content}`;
                }).join('\n\n');
                contextString += "Relevant Code Snippets:\n" + chunkContext;
                
                const uniqueFilePaths = new Set(contextChunks.map(c => {
                    return relevantFiles.find(f => f.id === c.file_id)?.path;
                }).filter((p): p is string => !!p));

                uniqueFilePaths.forEach(filePath => sources.push({
                    type: 'code',
                    id: filePath,
                    title: filePath
                }));
            }
        }
        
        if (!contextString.trim() && history.length === 0) {
            throw new Error("No relevant context found for this question (no tasks, commits, code, or existing conversation).");
        }
        
        client.release();
        client = null;

        // --- NEW: Prompt construction with history ---
        const systemPrompt = `You are an expert AI software engineer. Answer the user's question based ONLY on the provided context and conversation history. Context may include past decisions, project documents, tasks, commits, and code snippets. Be concise, accurate, and provide code snippets in Markdown format when relevant. If the context and history are insufficient, state that clearly.`;

        const userMessageWithContext = `CONTEXT:\n${contextString}\n\nQUESTION:\n${latestQuestion}`;

        // Construct the full message payload for the OpenAI API
        const messages: openAI.ChatMessage[] = [
            { role: 'system', content: systemPrompt },
            ...history,
            { role: 'user', content: userMessageWithContext }
        ];

        const stream = await openAI.getChatCompletionStream(messages);
        
        return { stream, sources };

    } finally {
        if (client) {
            client.release();
        }
    }
}

--- FILE: api/conversations/conversation.controller.ts ---

// --- FILE: api/conversations/conversation.controller.ts ---
import { Request, Response, NextFunction } from 'express';
import * as conversationService from './conversation.service';
import * as qaService from '../projects/qa.service'; 

// streamResponse function (unchanged)
async function streamResponse(req: Request, res: Response, conversationId: number, projectId: number) {
    // ... (previous code)
    // Setup SSE headers
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.flushHeaders();

    let fullResponse = '';
    
    req.on('close', () => {
        console.log(`Client disconnected from conversation stream ${conversationId}.`);
        res.end();
    });

    try {
        const messages = await conversationService.getConversationMessages(conversationId);

        if (messages.length === 0) {
            throw new Error('Cannot stream response for an empty conversation.');
        }

        // The last message is the current question from the user
        const lastMessage = messages[messages.length - 1];
        const question = lastMessage.content;

        // The preceding messages form the history
        const history: qaService.ChatMessage[] = messages.slice(0, -1).map(m => ({
            role: m.role as ('user' | 'assistant'),
            content: m.content
        }));

        const { stream, sources } = await qaService.getAnswerStream(
            projectId,
            question,
            history // <-- Pass the history here
        );
        
        res.write(`event: sources\ndata: ${JSON.stringify(sources)}\n\n`);

        for await (const chunk of stream) {
            if (res.writableEnded) break;
            const content = chunk.choices[0]?.delta?.content || '';
            if (content) {
                fullResponse += content;
                res.write(`event: token\ndata: ${JSON.stringify({ token: content })}\n\n`);
            }
        }

        await conversationService.addAssistantMessage(conversationId, fullResponse, sources);

        if (!res.writableEnded) {
            res.write(`event: end\ndata: ${JSON.stringify({ message: "Stream finished" })}\n\n`);
            res.end();
        }
    } catch (streamError) {
        const errorMessage = streamError instanceof Error ? streamError.message : "An unknown error occurred during streaming.";
        console.error("Error during Q&A stream:", streamError);
        if (!res.writableEnded) {
            res.write(`event: error\ndata: ${JSON.stringify({ message: errorMessage })}\n\n`);
            res.end();
        }
    }
}


// createConversation function (unchanged)
export async function createConversation(req: Request, res: Response, next: NextFunction) {
    // ... (previous code)
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const { first_message } = req.body;

        if (!first_message) {
            return res.status(400).json({ error: 'A "first_message" is required.' });
        }

        const conversation = await conversationService.createConversation(projectId, first_message);
        
        // MODIFIED: Pass req and projectId to the stream handler
        await streamResponse(req, res, conversation.id, projectId);

    } catch (error) {
        if (!res.headersSent) {
          next(error);
        } else {
          console.error("Error after headers sent in createConversation:", error);
          if (!res.writableEnded) {
            res.end();
          }
        }
    }
}

// addMessageToConversation function (unchanged)
export async function addMessageToConversation(req: Request, res: Response, next: NextFunction) {
    // ... (previous code)
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const conversationId = parseInt(req.params.conversationId, 10);
        const { message } = req.body;

        if (!message) {
            return res.status(400).json({ error: 'A "message" is required.' });
        }
        
        await conversationService.addUserMessage(conversationId, message);
        
        // MODIFIED: Pass req and projectId to the stream handler
        await streamResponse(req, res, conversationId, projectId);

    } catch (error) {
         if (!res.headersSent) {
          next(error);
        } else {
          console.error("Error after headers sent in addMessageToConversation:", error);
          if (!res.writableEnded) {
            res.end();
          }
        }
    }
}


// listConversations function (unchanged)
export async function listConversations(req: Request, res: Response, next: NextFunction) {
    // ... (previous code)
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const conversations = await conversationService.listConversations(projectId);
        res.json(conversations);
    } catch (error) {
        next(error);
    }
}

// getConversation function (unchanged)
export async function getConversation(req: Request, res: Response, next: NextFunction) {
    // ... (previous code)
    try {
        const conversationId = parseInt(req.params.conversationId, 10);
        const messages = await conversationService.getConversationMessages(conversationId);
        res.json(messages);
    } catch (error) {
        next(error);
    }
}

// NEW: Controller for capturing knowledge
export async function captureKnowledge(req: Request, res: Response, next: NextFunction) {
    try {
        const projectId = parseInt(req.params.projectId, 10);
        const conversationId = parseInt(req.params.conversationId, 10);

        const result = await conversationService.captureKnowledgeFromConversation(projectId, conversationId);

        if (!result) {
            return res.status(200).json({ message: 'No significant knowledge was found to capture.' });
        }

        res.status(201).json({ message: 'Knowledge captured successfully.', knowledgeNote: result });

    } catch (error) {
        next(error);
    }
}

--- FILE: api/conversations/conversation.routes.ts ---

// --- FILE: api/conversations/conversation.routes.ts ---
import { Router } from 'express';
import * as conversationController from './conversation.controller';

// mergeParams is crucial for nested routes to access parent params like :projectId
const router = Router({ mergeParams: true });

router.post('/', conversationController.createConversation);
router.get('/', conversationController.listConversations);
router.get('/:conversationId', conversationController.getConversation);
router.post('/:conversationId/messages', conversationController.addMessageToConversation);

// NEW: Route to trigger the knowledge capture process for a conversation.
router.post('/:conversationId/capture-knowledge', conversationController.captureKnowledge);

export default router;

--- FILE: api/conversations/conversation.service.ts ---

// --- FILE: api/conversations/conversation.service.ts ---
import pool from '../../services/db';
import * as openAI from '../../services/openai';
import pgvector from 'pgvector/pg';
import { Source } from '../projects/qa.service'; // Import the Source type

/**
 * Creates a new conversation and its first user message in the database.
 * @param projectId The ID of the project.
 * @param firstMessage The content of the user's first message.
 * @returns The newly created conversation object.
 */
export async function createConversation(projectId: number, firstMessage: string) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');

        // Generate a title from the first message
        const title = firstMessage.length > 80 ? firstMessage.substring(0, 77) + '...' : firstMessage;

        const convResult = await client.query(
            'INSERT INTO conversations (project_id, title) VALUES ($1, $2) RETURNING *',
            [projectId, title]
        );
        const conversation = convResult.rows[0];

        await client.query(
            `INSERT INTO conversation_messages (conversation_id, role, content) VALUES ($1, 'user', $2)`,
            [conversation.id, firstMessage]
        );

        await client.query('COMMIT');
        return conversation;
    } catch (error) {
        await client.query('ROLLBACK');
        console.error('Error creating conversation:', error);
        throw error;
    } finally {
        client.release();
    }
}

/**
 * Adds a new user message to an existing conversation.
 * @param conversationId The ID of the conversation.
 * @param message The content of the user's message.
 */
export async function addUserMessage(conversationId: number, message: string) {
    const client = await pool.connect();
    try {
        await client.query(
            `INSERT INTO conversation_messages (conversation_id, role, content) VALUES ($1, 'user', $2)`,
            [conversationId, message]
        );
    } finally {
        client.release();
    }
}

/**
 * Adds an assistant's response to a conversation.
 * @param conversationId The ID of the conversation.
 * @param content The full text content of the assistant's reply.
 * @param sources The sources cited in the reply.
 */
export async function addAssistantMessage(conversationId: number, content: string, sources: any) {
    const client = await pool.connect();
    try {
        await client.query(
            `INSERT INTO conversation_messages (conversation_id, role, content, sources) VALUES ($1, 'assistant', $2, $3)`,
            [conversationId, content, JSON.stringify(sources)]
        );
        // Also update the conversation's updated_at timestamp
        await client.query('UPDATE conversations SET updated_at = NOW() WHERE id = $1', [conversationId]);
    } finally {
        client.release();
    }
}

/**
 * Lists all conversations for a given project.
 * @param projectId The ID of the project.
 */
export async function listConversations(projectId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query(
            'SELECT id, title, updated_at FROM conversations WHERE project_id = $1 ORDER BY updated_at DESC',
            [projectId]
        );
        return rows;
    } finally {
        client.release();
    }
}

/**
 * Retrieves all messages for a single conversation.
 * @param conversationId The ID of the conversation.
 */
export async function getConversationMessages(conversationId: number) {
    const client = await pool.connect();
    try {
        const { rows } = await client.query(
            'SELECT * FROM conversation_messages WHERE conversation_id = $1 ORDER BY created_at ASC',
            [conversationId]
        );
        return rows;
    } finally {
        client.release();
    }
}

// NEW: The core "learning" function.
export async function captureKnowledgeFromConversation(projectId: number, conversationId: number): Promise<{ id: number; summary: string } | null> {
    const client = await pool.connect();
    try {
        const messages = await getConversationMessages(conversationId);
        if (messages.length < 2) { // Need at least one user message and one assistant reply
            return null;
        }

        const transcript = messages.map(m => `${m.role.toUpperCase()}:\n${m.content}`).join('\n\n---\n\n');

        const systemPrompt = `You are an AI assistant that distills key decisions and summaries from engineering conversations. Analyze the following transcript and extract the single most important decision, technical summary, or architectural choice. The summary MUST be a concise, one-sentence statement. If no clear decision was made or the conversation is trivial, respond with the exact string "NULL".

Example outputs:
- "Decision: The JWT expiration will be changed from 1 hour to 24 hours to improve user experience."
- "Conclusion: The ingestion pipeline performance issue is caused by a missing index on the 'commits' table."
- "Architectural Choice: A queueing system will be implemented using 'p-queue' to manage concurrent ingestion tasks."`;
        
        const userPrompt = `CONVERSATION TRANSCRIPT:\n\n${transcript}`;

        const summary = await openAI.getChatCompletion([
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
        ]);

        if (!summary || summary.trim().toUpperCase() === 'NULL') {
            console.log(`AI determined no knowledge could be captured from conversation ${conversationId}.`);
            return null;
        }
        
        await client.query('BEGIN');
        
        const embedding = await openAI.getEmbedding(summary);

        const noteResult = await client.query(
            'INSERT INTO knowledge_notes (project_id, conversation_id, note_summary, embedding) VALUES ($1, $2, $3, $4) RETURNING id',
            [projectId, conversationId, summary, pgvector.toSql(embedding)]
        );
        const knowledgeNoteId = noteResult.rows[0].id;
        
        // --- Link the knowledge to its sources ---
        const allSources: Source[] = messages
            .filter(m => m.role === 'assistant' && m.sources)
            .flatMap(m => m.sources);
        
        const uniqueSources = Array.from(new Map(allSources.map(s => [`${s.type}:${s.id}`, s])).values());

        for (const source of uniqueSources) {
            if (source.type === 'code') {
                const { rows } = await client.query('SELECT id FROM indexed_files WHERE path = $1 AND project_id = $2', [source.id, projectId]);
                if (rows.length > 0) {
                    await client.query('INSERT INTO knowledge_note_links (knowledge_note_id, file_id) VALUES ($1, $2)', [knowledgeNoteId, rows[0].id]);
                }
            } else if (source.type === 'task') {
                 const { rows } = await client.query('SELECT id FROM tasks WHERE task_number = $1 AND project_id = $2', [source.id, projectId]);
                 if (rows.length > 0) {
                    await client.query('INSERT INTO knowledge_note_links (knowledge_note_id, task_id) VALUES ($1, $2)', [knowledgeNoteId, rows[0].id]);
                }
            }
            // Add linking for commits, documents, etc. in the same fashion if needed
        }

        await client.query('COMMIT');

        console.log(`Successfully captured knowledge note ${knowledgeNoteId} from conversation ${conversationId}.`);
        return { id: knowledgeNoteId, summary };

    } catch (error) {
        await client.query('ROLLBACK');
        console.error(`Failed to capture knowledge from conversation ${conversationId}:`, error);
        throw error;
    } finally {
        client.release();
    }
}

--- FILE: services/db.ts ---

// src/services/db.ts
import { Pool } from 'pg';
import pgvector from 'pgvector/pg';

const connectionString = process.env.DATABASE_URL!;

if (!connectionString) {
    throw new Error("FATAL: Missing environment variable DATABASE_URL");
}

// Create a single, shared pool for the entire application
const pool = new Pool({ connectionString });

// Add a listener to register the vector type on each new connection
// that the pool creates.
pool.on('connect', async (client) => {
    await pgvector.registerType(client);
});

export default pool;

--- FILE: services/git.ts ---

// src/services/git.ts
import simpleGit from 'simple-git';
import { promises as fs } from 'fs';
import path from 'path';
import os from 'os';

const WORKSPACE_DIR = path.join(os.homedir(), '.ai-brain-workspace');
// Optional: Define a logger type for clarity
type GitLogger = (message: string) => void;


export function getWorkspacePathFromUrl(url: string): string {
    try {
        const parsedUrl = new URL(url);
        const cleanPath = (parsedUrl.hostname + parsedUrl.pathname).replace(/\.git$/, '');
        return path.join(WORKSPACE_DIR, cleanPath);
    } catch (e) {
        const sshMatch = url.match(/git@([^:]+):(.*)/);
        if (sshMatch) {
            const host = sshMatch[1];
            const repoPath = sshMatch[2].replace(/\.git$/, '');
            return path.join(WORKSPACE_DIR, host, repoPath);
        }
        return path.join(WORKSPACE_DIR, url.replace(/[^a-zA-Z0-9]/g, '_'));
    }
}

export async function cloneOrPullRepo(source: string, logger: GitLogger = console.log): Promise<string> {
    const projectPath = getWorkspacePathFromUrl(source);
    await fs.mkdir(WORKSPACE_DIR, { recursive: true });

    try {
        await fs.access(path.join(projectPath, '.git'));
        logger(`Found existing repository. Fetching updates from ${source}...`);
        await simpleGit(projectPath).pull();
        logger(`-> Updates pulled successfully.`);
    } catch (error) {
        logger(`Cloning repository from ${source}...`);
        await simpleGit().clone(source, projectPath);
        logger(`-> Cloned successfully.`);
    }
    return projectPath;
}

--- FILE: services/openai.ts ---

// src/services/openai.ts
import OpenAI from 'openai';

const openaiApiKey = process.env.OPENAI_API_KEY!;
if (!openaiApiKey) {
    throw new Error("FATAL: Missing environment variable OPENAI_API_KEY");
}

const openai = new OpenAI({ apiKey: openaiApiKey });

// NEW: Define a reusable type for chat messages
export interface ChatMessage {
    role: 'system' | 'user' | 'assistant';
    content: string;
}


export async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.replace(/\n/g, ' '),
  });
  return response.data[0].embedding;
}

// MODIFIED: This function now accepts a flexible array of messages
export async function getChatCompletionStream(messages: ChatMessage[]) {
    return openai.chat.completions.create({
        model: 'gpt-4o',
        messages: messages,
        stream: true,
    });
}

// NEW: A non-streaming version for tasks like summarization.
export async function getChatCompletion(messages: ChatMessage[]): Promise<string | null> {
    const response = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: messages,
    });
    return response.choices[0].message.content;
}

--- FILE: services/queue.service.ts ---

// --- FILE: services/queue.service.ts ---
import PQueue from 'p-queue';

// Create a single, shared queue for the entire application.
// concurrency: 1 ensures that only one promise runs at a time.
// This is our lock to prevent multiple ingestions from running simultaneously.
const ingestionQueue = new PQueue({ concurrency: 1 });

export default ingestionQueue;